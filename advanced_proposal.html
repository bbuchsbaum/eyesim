<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NA • eyesim</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="NA"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">eyesim</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="articles/Multimatch.html">Multimatch</a></li>
    <li><a class="dropdown-item" href="articles/Overview.html">Overview</a></li>
    <li><a class="dropdown-item" href="articles/RepetitiveSimilarity.html">Repetitive Similarity</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NA</h1>

    </div>


<p>Okay, this peer review provides excellent, actionable feedback. Below is the complete revised proposal, integrating all suggestions to create a robust and transparent plan for advancing eye-movement similarity analysis.</p>
<div class="section level2">
<h2 id="revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies">Revised Proposal: Advancing Eye-Movement Similarity Analysis for Encoding-Retrieval Studies<a class="anchor" aria-label="anchor" href="#revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies"></a></h2>
<p><strong>1. Introduction &amp; Motivation</strong></p>
<p>Current methods for quantifying eye-movement similarity between encoding and retrieval (e.g., cosine similarity on single-scale Fixation Density Maps - FDMs) offer valuable insights but may lack the sensitivity to capture the full richness of gaze reinstatement. This proposal outlines a staged approach to implement and rigorously evaluate increasingly powerful techniques for measuring eye-movement similarity. Our goal is to enhance sensitivity, improve interpretability where possible, and establish a data-driven framework for selecting the optimal metric(s) for understanding gaze-based memory reinstatement.</p>
<p>We will pay close attention to <strong>stimulus similarity foils</strong>, evaluating metric performance not just on identical image pairs but also on same-category, different-instance pictures to ensure our measures capture specific memory reinstatement rather than generic saliency overlap.</p>
<p><strong>1.1. Gaze-Data Quality Control</strong> Prior to any similarity analysis, rigorous data quality control will be implemented: * <strong>Track-Loss Handling:</strong> Frames with &gt;[X]% track loss within a [Y]ms window (or equivalent pupil-size validity metrics) will be identified. Short periods of track loss (&lt;[Z]ms) may be interpolated if appropriate for the analysis stage; longer periods will lead to fixation exclusion or trial rejection. * <strong>Exclusion Thresholds:</strong> Trials with &lt;[N] valid fixations or total gaze data below [P]% of trial duration will be excluded. Participants with &gt;[Q]% trial exclusion will be considered for removal from the analysis. * <strong>Calibration:</strong> Re-calibration procedures will be performed between encoding and retrieval blocks (or at fixed intervals) to maintain high spatial accuracy. Drift correction will be applied if necessary.</p>
<p><strong>2. Overarching Strategy &amp; Guiding Principles</strong></p>
<p>Our approach is built on the following principles:</p>
<ul><li>
<strong>Incremental Complexity:</strong> We will progress from simpler, drop-in upgrades to more sophisticated, novel methods, ensuring each step justifies its added complexity.</li>
<li>
<strong>Quantitative Gating:</strong> Pre-defined statistical thresholds for improvement will guide decisions to proceed to more complex stages.</li>
<li>
<strong>Interpretability Balance:</strong> While seeking maximum sensitivity, we will prioritize interpretable methods and include strategies to “peek inside” more black-box approaches.</li>
<li>
<strong>Rigorous Validation:</strong> A comprehensive validation framework, including noise ceiling estimation and qualitative error analysis, will be applied consistently.</li>
<li>
<strong>Participant-Level Sensitivity:</strong> We will explore and account for individual differences in optimal metric parameters.</li>
<li>
<strong>Task Variant Consideration:</strong> This proposal assumes [e.g., full-image re-presentation during retrieval]. If multiple task variants are used (e.g., full-image vs. partial-cue delay), metrics will be evaluated separately for each. Metrics will only be pooled across tasks if a pilot analysis demonstrates that metric × task interactions are negligible for the primary outcomes.</li>
</ul><p><strong>3. Methodological Stages &amp; Evaluation</strong></p>
<p><em>(A graphical concept figure will be developed to visually illustrate scanpaths and their representation at each stage: FDM, multi-scale FDM, DTW path, graph representation. This will appear as Figure 1 in the final methods.)</em></p>
<p><strong>Stage 0: Establish Current Baseline</strong> * <em>(Effort ≈ 1 week; RAM ≈ 4 GB)</em> * <strong>Method:</strong> Re-implement the current standard: Fixation Density Map (FDM) smoothed by a single σ, with cosine similarity. * <strong>Evaluation:</strong> Reproduce published/internal effect sizes for key behavioural correlates (e.g., recognition accuracy, vividness). This serves as the primary benchmark.</p>
<p><strong>Stage 1: Upgrade the Heat-Map Metric (Same Representation, Better Distance)</strong> * <em>(Effort ≈ 2 weeks; RAM ≈ 8 GB)</em> * <strong>Improvements:</strong> 1. <strong>Earth-Mover’s Distance (EMD/Wasserstein-1):</strong> Replace cosine similarity with EMD between FDMs. Optionally, sign EMD by subtracting a per-image saliency map (e.g., using a pre-trained model like DeepGaze III, ITTI, or similar, specified <em>a priori</em>). Scene-dependent failures of the saliency model will be monitored during the qualitative error-triage. * <em>Tool:</em> <code>transport::wasserstein()</code> in R. 2. <strong>Multi-Scale Similarity:</strong> Compute FDMs at multiple spatial scales (e.g., σ = 0.25°, 0.5°, 1.0°) and average the (1 – EMD) similarity across scales. 3. <strong>Duration-Weighted Density:</strong> Weight fixations by their duration (or inverse saccade velocity) when creating density maps. * <strong>Evaluation:</strong> * Compare multi-scale, duration-weighted EMD against Stage 0 using permutation z-scores and ΔAUC (macro) for same-image classification. * <strong>Quantitative Gate:</strong> Proceed if ΔAUC ≥ <code>max(0.02, 0.05 * baseline_AUC_standard_error)</code> and the 95% CI of ΔAUC &gt; 0. * A power analysis (via simulation) will be pre-registered to ensure adequate power (e.g., 80%) to detect this ΔAUC gate with the expected number of participant-item pairs. * <strong>Interpretability:</strong> EMD remains relatively interpretable.</p>
<p><strong>Stage 2: Bring Back Time – Soft Sequence Alignment</strong> * <em>(Effort ≈ 2-3 weeks; RAM ≈ 8-16 GB)</em> * <strong>Improvements:</strong> 1. <strong>Soft Dynamic Time Warping (soft-DTW):</strong> Apply soft-DTW to raw (x,y) fixation sequences. The DTW cost function (e.g., Euclidean distance) will be fixed <em>a priori</em>. * <em>Tool:</em> <code>dtwclust</code> or <code>tsclust</code> in R. 2. <strong>(Alternative) Temporal Cross-Recurrence Plot (CRP):</strong> Summarise diagonality from a binary matrix of pairwise distances &lt; ε between encoding and retrieval fixations. The bandwidth ε will be fixed <em>a priori</em>. 3. <strong>Combined Metric:</strong> Fuse the spatial metric from Stage 1 with the temporal metric (soft-DTW distance) using a simple weighted sum. * <strong>Evaluation:</strong> * Compare the Stage 1 + soft-DTW fusion against Stage 1 alone. Assess impact on behavioural correlations using mixed-effects models. * <strong>Quantitative Gate:</strong> Proceed if Δβ (standardized, for vividness prediction) ≥ +0.10 or p &lt; 0.01 (Holm-corrected) for the interaction term or main effect of the improved metric. * <strong>Interpretability:</strong> DTW alignment paths and CRPs offer visual insights.</p>
<p><strong>Stage 3: Graph-Theoretic Representation (Novel &amp; Interpretable Features)</strong> * <em>(Effort ≈ 1 month; RAM ≈ 30-40 GB for kernels on ~10k pairs; HPC/GPU access planned if needed)</em> * <strong>Representation:</strong> Convert each scanpath into an undirected weighted graph (Nodes = Fixation centroids; Edge weight = Saccade probability). * <strong>Similarity Metrics/Features:</strong> 1. Global Similarity (Graph Edit Distance, Delta-Con). 2. Graph Kernels (e.g., Weisfeiler-Lehman) fed to an SVM. 3. Extracted Interpretable Features (Degree centrality Gini, Edge-betweenness of AOIs, Triad census). * <em>Tool:</em> <code>igraph</code> in R, <code>graphkernels</code> via reticulate. * <strong>Evaluation:</strong> * Compare a model using EMD + DTW + extracted graph features against Stage 2. * Test using 10-fold nested CV on same-image classification. * Demonstrate unique predictive variance using hierarchical variance-partitioning (LRT), cross-validated ΔAUC decomposition, collinearity checks (VIF &lt; 3), and feature importance. * <strong>Quantitative Gate:</strong> Proceed to Stage 4 if Δχ² (LRT for graph features) shows p &lt; 0.005. * <strong>Interpretability:</strong> Graph features are directly interpretable; kernels can be probed.</p>
<p><strong>Stage 4: Contrastive Metric Learning with a Siamese ScanPath Encoder (Powerful &amp; New)</strong> * <em>(Effort ≈ 1-2 months; RAM ≈ 30-40 GB for training on ~10k pairs; HPC/GPU access planned)</em> * <strong>Representation:</strong> Sequence of fixations (x, y, duration, velocity) → positional embeddings → small Transformer or 1D CNN. * <strong>Loss Function:</strong> InfoNCE / Triplet loss. * <strong>Output:</strong> Learned embedding space where cosine similarity reflects scanpath similarity. * <em>Tool:</em> <code>torch</code> or <code>keras</code> in R. * <strong>Decision Rule &amp; Data Requirements:</strong> “Siamese encoder proceeds if Stage 3 &lt; 85 % of picture-ID ceiling (see Section 4.1) and sample size ≥ 3,000 positive pairs.” Heavy data augmentation will be used. * <strong>Evaluation:</strong> Compare learned similarity against Stage 3. * <strong>Interpretability Probes:</strong> t-SNE plots, Gradient × Input, frozen backbone + GLM.</p>
<p><strong>Stage 5: Hybrid “Gaze-Conditioned Optimal Transport” (Blue-Sky Idea)</strong> * <em>(Effort ≈ 1 month (if pursued); RAM dependent on implementation, likely &gt;16GB)</em> * <strong>Representation:</strong> Fixation as 5-D point: (x, y, t, Δt, v). * <strong>Similarity:</strong> 5-D Wasserstein distance with optimized λ_t, λ_v per participant. * <strong>Evaluation:</strong> Ablation tests (λ_v=0 vs. optimized), variance-partitioning.</p>
<p><strong>4. Cross-Cutting Validation, Power Boost, and Decision Framework</strong></p>
<p><strong>4.1. Estimating the “Noise Ceiling” / Classification Head-Room</strong> To inform the 85% decision rule for Stage 4:</p>
<ol style="list-style-type: decimal"><li>
<strong>Split-Half Scanpath Reliability:</strong> As previously described.</li>
<li>
<strong>Oracle Classifiers:</strong>
<ul><li>
<strong>Picture-ID Oracle:</strong> (Leave-picture-out CV).</li>
<li>
<strong>Subject-ID Oracle:</strong> (Leave-subject-out CV).</li>
<li>
<strong>Joint Oracle:</strong> (Random 80/20 split).</li>
<li><em>All three will be reported. The “effective” ceiling for cross-subject generalization will be considered the lower of the picture-ID oracle and the GBT proxy to avoid inflation by subject fingerprints.</em></li>
</ul></li>
<li>
<strong>Bayes Error Proxy (GBT):</strong> XGBoost on flattened fixation sequences (first 30 enc + 30 ret, NA padding, plus fixation counts).</li>
</ol><p><strong>Rule Update for Stage 4:</strong> The multi-scale-EMD + soft-DTW (Stage 2) or graph-based model (Stage 3) performance will be compared against 85% of the <strong>Picture-ID Oracle ceiling</strong> (or the GBT proxy if lower and deemed more appropriate for generalization).</p>
<p><strong>4.2. General Validation Tools</strong></p>
<ul><li>
<strong>Permutation Framework:</strong> As previously described (z-scores).</li>
<li>
<strong>Multi-Metric Fusion:</strong> “Multi-metric fusion will be explored only after single-metric benchmarking to avoid curse-of-dimensionality.” If pursued, logistic regression or random forest will be used.</li>
<li>
<strong>Reliability Check:</strong> As previously described (split-half).</li>
<li>
<strong>Net Re-classification Improvement (NRI):</strong> Pre-registered as a secondary criterion for model comparison, guarding against cases where rank order improves (AUC) but calibration worsens.</li>
</ul><p><strong>4.3. Systematic Participant-Level Optimization &amp; Heterogeneity</strong> “Participant-level optimisation: subject-specific σ/ω (and other relevant parameters like λ for Stage 5) become random slopes in mixed-effects models; inference reported at group level after partial pooling (e.g., via hierarchical Bayesian models if tailoring shows significant benefit via LRT comparison of random slope vs. fixed slope models).” 1. Per-subject hyper-parameter grid optimization. 2. Random-slopes mixed models. 3. Report heterogeneity of optimal parameters and correlate with traits. 4. Shrinkage estimators (hierarchical Bayesian models) if tailoring helps.</p>
<p><strong>4.4. Qualitative Error Triage (Embedded at Stage Ends)</strong> A systematic qualitative audit of “difficult” trials: 1. Identify discrepant pairs (top false positives/negatives; high-vividness/low-similarity quartiles). 2. Visual diagnostics (dual heat-maps, animated replays, AOI strings). 3. Annotation worksheet for two coders (tagging factors like partial cues, central bias, idiosyncratic strategy). 4. Feedback loop to inform metric tweaks. A tiny <strong>Shiny dashboard</strong> will be developed early on (heat-map overlay, GIF replay, coding form) to streamline this process and reduce coder burden; this tool will be mentioned in the Methods. A “Qualitative Error Analysis” subsection will feature representative vignettes.</p>
<p><strong>5. Computational Logistics &amp; Reproducibility</strong></p>
<ul><li>
<strong>Tool Chain:</strong> R (<code>transport</code>, <code>dtwclust</code>, <code>igraph</code>, <code>targets</code> for workflow management), Python via <code>reticulate</code> (<code>graphkernels</code>), <code>torch</code>/<code>keras</code> (for R or Python).</li>
<li>
<strong>Hardware Budget:</strong> Explicit RAM and potential HPC/GPU needs noted per stage.</li>
<li>
<strong>Data Storage &amp; Pre-computation:</strong> Large O(n²) similarity matrices will be managed, potentially with HDF5 or <code>fst</code> for efficient caching.</li>
<li>
<strong>Data Versioning:</strong> A data-versioning plan (e.g., DVC - Data Version Control, or Git-LFS for key matrices/CSVs) will be implemented to ensure pre-computed similarity matrices and intermediate datasets are traceable and shareable.</li>
<li>
<strong>Reproducibility:</strong> A minimal Docker/Singularity recipe will be provided in an Appendix to facilitate turnkey reproducibility of the computational environment.</li>
</ul><p><strong>6. Implementation Roadmap &amp; Quantitative Gates</strong></p>
<table class="table"><colgroup><col width="4%"><col width="25%"><col width="70%"></colgroup><thead><tr class="header"><th align="left">Month(s)</th>
<th align="left">Key Milestones</th>
<th align="left">Quantitative Gate / Action Item</th>
</tr></thead><tbody><tr class="odd"><td align="left"><strong>1</strong></td>
<td align="left">Stage 0 (Baseline). Implement Stage 1 (EMD). Gaze QC.</td>
<td align="left">ΔAUC (S1 vs S0) ≥ <code>max(0.02, 0.05*baseline_SE)</code> &amp; 95% CI &gt; 0. Compute oracles; update 85% rule. Power analysis for S1 gate.</td>
</tr><tr class="even"><td align="left"><strong>1.5-2</strong></td>
<td align="left">Stage 1 Eval &amp; Error Triage (Shiny app ready). Implement Stage 2 (soft-DTW).</td>
<td align="left">Δβ (vividness, S2 vs S1) ≥ +0.10 or p &lt; 0.01 (Holm). Run GBT proxy.</td>
</tr><tr class="odd"><td align="left"><strong>2.5-3</strong></td>
<td align="left">Stage 2 Eval &amp; Error Triage. Implement Stage 3 (Graph features). Define DTW cost/ε.</td>
<td align="left">Δχ² (LRT for graph feat. vs S2) p &lt; 0.005. VIF &lt; 3. NRI check.</td>
</tr><tr class="even"><td align="left"><strong>3.5-4</strong></td>
<td align="left">Stage 3 Eval &amp; Error Triage. Assess participant-level optimization.</td>
<td align="left">LRT p &lt; 0.01 favoring random slopes. <strong>Decision Point:</strong> Stage 4 if &lt;85% of (Picture-ID oracle OR GBT proxy, whichever is lower &amp; relevant for generalization) ceiling met AND sample size ≥ 3k positive pairs. NRI check.</td>
</tr><tr class="odd"><td align="left"><strong>4+ (If needed)</strong></td>
<td align="left">Prototype Stage 4 (Siamese). Stage 5 (5D OT) consideration.</td>
<td align="left">Compare to S3. Assess remaining noise-ceiling gap. NRI check.</td>
</tr><tr class="even"><td align="left"><strong>Ongoing</strong></td>
<td align="left">Qualitative Error Triage packets produced at each stage end.</td>
<td align="left">Analyst notes and visual diagnostics via Shiny app.</td>
</tr></tbody></table><p><strong>7. Conclusion &amp; Expected Outcomes</strong></p>
<p>This systematic, multi-stage proposal provides a clear path to significantly enhance the analysis of eye-movement similarity. By combining incremental advancements with rigorous, quantitatively-gated evaluations, qualitative insights, and a commitment to open science, we aim to:</p>
<ul><li>Develop more sensitive and nuanced measures of gaze reinstatement.</li>
<li>Understand the contributions of spatial, temporal, and dynamic gaze features.</li>
<li>Identify when complex models offer benefits over simpler ones.</li>
<li>Provide a robust framework adaptable for future research.</li>
</ul><p><strong>Dissemination Plan:</strong> We commit to open science practices. This will include: * Pre-registration of the study plan (e.g., on OSF). * Making analysis code open-source (e.g., via GitHub). * Sharing anonymized data and pre-computed similarity matrices where ethically permissible and feasible. * Publishing findings in peer-reviewed journals and presenting at conferences, with a preprint made available (e.g., on bioRxiv/PsyArXiv).</p>
<p>Okay, I will add this explanation as an addendum to the main proposal.</p>
</div>
<div class="section level2">
<h2 id="revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies-1">Revised Proposal: Advancing Eye-Movement Similarity Analysis for Encoding-Retrieval Studies<a class="anchor" aria-label="anchor" href="#revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies-1"></a></h2>
<p><strong>(Main proposal content as previously detailed remains here – from Section 1. Introduction &amp; Motivation through Section 7. Conclusion &amp; Expected Outcomes)</strong></p>
<hr><p><strong>Addendum: Explanation of “Signing” the Earth-Mover’s Distance (EMD)</strong></p>
<p>This addendum clarifies the concept and implementation of “signing” the Earth-Mover’s Distance, a technique optionally employed in Stage 1 of the proposed methodology to refine eye-movement similarity measures by accounting for bottom-up visual saliency.</p>
<p><strong>A.1. Plain-English Idea</strong></p>
<p>Eye-tracking heat maps are naturally all-positive probability distributions: every pixel holds non-negative fixation density, and the whole map sums to 1. The vanilla Earth-Mover Distance (EMD) between two such maps is always non-negative—it only tells you how much work it takes to morph one distribution into the other.</p>
<p>However, suppose both observers (e.g., during encoding and retrieval of the same image) spent a lot of time on regions that were already highly bottom-up salient (e.g., bright areas, high-contrast corners, faces). A significant portion of the measured EMD might simply reflect this shared “saliency gravity,” rather than the idiosyncratic, memory-driven component of gaze that is often of primary interest in cognitive studies.</p>
<p>By subtracting a saliency baseline map first, each fixation heat map is converted into a <em>signed residual map</em>. In these residual maps: * Positive values indicate regions where the viewer looked <em>more</em> than predicted by bottom-up saliency. * Negative values indicate regions where the viewer looked <em>less</em> than predicted by bottom-up saliency.</p>
<p>Computing EMD on these residual maps, therefore, measures how similarly the two scanpaths <em>depart from</em> the baseline saliency predictions. The resulting score can be positive or negative (hence “signed EMD”), reflecting the nature and similarity of these departures.</p>
<p><strong>A.2. Step-by-Step Recipe for Signed EMD</strong></p>
<ol style="list-style-type: decimal"><li>
<strong>Compute a Saliency Map for Each Stimulus:</strong>
<ul><li>
<code>S(x,y)</code> ← Prediction from a pre-specified saliency model (e.g., DeepGaze III, ITTI &amp; Koch, or another chosen <em>a priori</em> model), normalized to integrate to 1. The choice of saliency model will be documented, and its potential scene-dependent failures noted as an area for monitoring during the qualitative error-triage.</li>
</ul></li>
<li>
<strong>Generate Standard Fixation-Density Maps (FDMs):</strong>
<ul><li>
<code>F_enc(x,y)</code> (for encoding) and <code>F_ret(x,y)</code> (for retrieval), each smoothed with the same kernel (as defined in Stage 1) and normalized to integrate to 1.</li>
</ul></li>
<li>
<strong>Form Signed Residual Maps:</strong>
<ul><li><code>R_enc = F_enc - S</code></li>
<li><code>R_ret = F_ret - S</code></li>
<li>These maps now represent the deviation of observed gaze from saliency predictions. The sum of pixel values in a residual map is zero.</li>
</ul></li>
<li>
<strong>Split Residuals into Positive and Negative “Piles of Earth”:</strong> For each residual map (e.g., <code>R_enc</code>):
<ul><li><code>R_enc_positive(x,y) = max(R_enc(x,y), 0)</code></li>
<li>
<code>R_enc_negative(x,y) = max(-R_enc(x,y), 0)</code> And similarly for <code>R_ret</code>.</li>
<li>Each of these <code>R_positive</code> and <code>R_negative</code> maps now forms a proper, non-negative distribution. The total mass (sum of pixel values) in <code>R_positive</code> will equal the total mass in <code>R_negative</code> for a given original residual map (because the positive and negative parts of the residual map <code>R</code> must cancel each other out as <code>R</code> sums to zero).</li>
</ul></li>
<li>
<strong>Compute Two EMDs:</strong>
<ul><li>
<code>EMD_positive</code> = EMD between <code>R_enc_positive</code> and <code>R_ret_positive</code>.</li>
<li>
<code>EMD_negative</code> = EMD between <code>R_enc_negative</code> and <code>R_ret_negative</code>.</li>
<li>These measure the “work” required to transform the “looked more than saliency” patterns into each other, and similarly for the “looked less than saliency” patterns.</li>
</ul></li>
<li>
<strong>Derive a Single Signed Similarity Score:</strong>
<ul><li><code>SignedEMD = -(EMD_positive + EMD_negative)</code></li>
<li>
<strong>Interpretation:</strong>
<ul><li>Small (more negative) values indicate a large total EMD cost, meaning the two observers departed from saliency in dissimilar ways.</li>
<li>Large (less negative or even positive, approaching zero) values indicate a low total EMD cost, meaning the two observers over-shot and under-shot saliency in similar locations, suggesting genuine reinstatement of gaze patterns beyond bottom-up pull.</li>
</ul></li>
<li><em>(Note: The sign convention is arbitrary as long as it’s consistent. Some researchers might report the negative of the total cost so that “higher = more similar.” The chosen convention will be clearly stated.)</em></li>
</ul></li>
</ol><p><strong>A.3. Why Bother with Signed EMD?</strong></p>
<ul><li>
<strong>Controls for Bottom-Up Bias:</strong> If both scanpaths align with highly salient features simply because those features attract everyone’s gaze, subtracting the saliency map neutralizes this shared variance, allowing for a cleaner measure of top-down influences.</li>
<li>
<strong>Highlights Mnemonic or Task-Driven Guidance:</strong> The residual gaze patterns (regions looked at more or less than predicted by saliency) are stronger candidates for reflecting top-down, memory-driven, or task-relevant fixation choices.</li>
<li>
<strong>Produces an Effect Direction (Potentially):</strong> While the sum of two non-negative EMDs is always non-negative, the specific formulation and interpretation focus on the <em>similarity of deviations</em>. The term “signed” primarily refers to the residual maps themselves. The final similarity score, as defined above, will range from highly negative (very dissimilar deviations) towards zero (very similar deviations).</li>
</ul><p><strong>A.4. Quick R Pseudo-Code Example</strong></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Assume F_enc, F_ret, and S are matrices (e.g., from imager::as.cimg())</span></span>
<span><span class="co"># representing fixation density maps and the saliency map,</span></span>
<span><span class="co"># all appropriately smoothed and normalized to sum to 1.</span></span>
<span></span>
<span><span class="co"># library(imager) # for image handling, pmax</span></span>
<span><span class="co"># library(transport) # for wasserstein()</span></span>
<span></span>
<span><span class="co"># 1. Saliency map S is pre-computed</span></span>
<span><span class="co"># 2. F_enc and F_ret are generated</span></span>
<span></span>
<span><span class="co"># 3. Form signed residual maps</span></span>
<span><span class="va">res_enc</span> <span class="op">&lt;-</span> <span class="va">F_enc</span> <span class="op">-</span> <span class="va">S</span></span>
<span><span class="va">res_ret</span> <span class="op">&lt;-</span> <span class="va">F_ret</span> <span class="op">-</span> <span class="va">S</span></span>
<span></span>
<span><span class="co"># 4. Split residuals into positive and negative "piles"</span></span>
<span><span class="va">pos_enc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmax</a></span><span class="op">(</span><span class="va">res_enc</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">neg_enc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmax</a></span><span class="op">(</span><span class="op">-</span><span class="va">res_enc</span>, <span class="fl">0</span><span class="op">)</span> <span class="co"># Note: -res_enc, so positive values here represent where original was negative</span></span>
<span></span>
<span><span class="va">pos_ret</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmax</a></span><span class="op">(</span><span class="va">res_ret</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">neg_ret</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmax</a></span><span class="op">(</span><span class="op">-</span><span class="va">res_ret</span>, <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Ensure these are suitable for transport::wasserstein (e.g., as 2D histograms or point patterns)</span></span>
<span><span class="co"># This might require converting matrix representations to the format expected by the function.</span></span>
<span><span class="co"># For 2D histograms, transport::wasserstein() expects inputs that are themselves</span></span>
<span><span class="co"># representations of distributions (e.g., objects from hist() or similar structures).</span></span>
<span><span class="co"># If F_enc etc. are simple matrices, they might need to be converted.</span></span>
<span><span class="co"># For simplicity, let's assume they can be directly used or easily converted.</span></span>
<span></span>
<span><span class="co"># 5. Compute two EMDs (using p=1 for Wasserstein-1 distance)</span></span>
<span><span class="co"># The exact call to wasserstein() will depend on how pos_enc etc. are structured.</span></span>
<span><span class="co"># Assuming they are compatible 2D histograms / distributions:</span></span>
<span><span class="va">emd_pos</span> <span class="op">&lt;-</span> <span class="fu">transport</span><span class="fu">::</span><span class="fu">wasserstein</span><span class="op">(</span><span class="fu">pp</span><span class="op">(</span><span class="va">pos_enc</span><span class="op">)</span>, <span class="fu">pp</span><span class="op">(</span><span class="va">pos_ret</span><span class="op">)</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># pp() might be a placeholder for conversion</span></span>
<span><span class="va">emd_neg</span> <span class="op">&lt;-</span> <span class="fu">transport</span><span class="fu">::</span><span class="fu">wasserstein</span><span class="op">(</span><span class="fu">pp</span><span class="op">(</span><span class="va">neg_enc</span><span class="op">)</span>, <span class="fu">pp</span><span class="op">(</span><span class="va">neg_ret</span><span class="op">)</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># to point pattern or suitable histogram format</span></span>
<span></span>
<span><span class="co"># 6. Derive a single signed similarity score</span></span>
<span><span class="va">signed_similarity_score</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="op">(</span><span class="va">emd_pos</span> <span class="op">+</span> <span class="va">emd_neg</span><span class="op">)</span> <span class="co"># Higher values (closer to 0) mean more similar deviations</span></span>
<span></span>
<span><span class="co"># This signed_similarity_score is then used in subsequent analyses</span></span>
<span><span class="co"># (permutation framework, mixed models, etc.) in place of the vanilla EMD.</span></span></code></pre></div>
<p><em>(The <code>pp()</code> in the pseudo-code is a placeholder; actual conversion to the input format required by <code>transport::wasserstein</code> for 2D histograms would be needed, e.g., ensuring they are structured as outputs from <code>hist</code> or as two-column matrices of coordinates and masses for point patterns.)</em></p>
<p><strong>A.5. Take-Home Message</strong></p>
<p>“Signing the EMD” is a methodological refinement that aims to isolate the cognitive component of gaze similarity. By first correcting each fixation map for baseline, bottom-up visual saliency, the subsequent EMD calculation reflects how similarly two gaze patterns deviate from these predictions. This approach provides a more targeted measure of memory-driven or task-relevant gaze reinstatement, rather than simply quantifying the overlap in raw saliency-influenced gaze distributions. The resulting <code>signed_similarity_score</code> is then used within the same statistical framework as other similarity metrics.</p>
<p>Okay, I will add a second addendum detailing how to wrap DeepGaze in R via <code>reticulate</code>, including a helper function for installation. This assumes a Python environment with DeepGaze III (or a similar version) is accessible or can be set up.</p>
<p><strong>(Main proposal content as previously detailed remains here – from Section 1. Introduction &amp; Motivation through Section 7. Conclusion &amp; Expected Outcomes, followed by Addendum A: Explanation of “Signing” the Earth-Mover’s Distance (EMD))</strong></p>
<hr><p><strong>Addendum B: Integrating DeepGaze Saliency Model in R via Reticulate</strong></p>
<p>This addendum provides a practical guide for using a Python-based saliency model like DeepGaze III within an R workflow using the <code>reticulate</code> package. This is relevant for Stage 1 when optionally subtracting a per-image saliency map to compute Signed EMD.</p>
<p><strong>B.1. Overview</strong></p>
<p>DeepGaze models are typically implemented in Python using deep learning frameworks like PyTorch or TensorFlow. The <code>reticulate</code> package in R allows for seamless interoperability between R and Python, enabling us to call Python functions and use Python objects directly from R.</p>
<p><strong>B.2. Prerequisites</strong></p>
<ol style="list-style-type: decimal"><li>
<strong>Python Installation:</strong> A working Python installation (e.g., Python 3.7+) is required.</li>
<li>
<strong>R and Reticulate:</strong> R and the <code>reticulate</code> package must be installed (<code>install.packages("reticulate")</code>).</li>
<li>
<strong>Python Environment for DeepGaze:</strong> It’s highly recommended to create a dedicated Python virtual environment (e.g., using <code>conda</code> or <code>venv</code>) to manage DeepGaze and its dependencies without conflicting with other Python projects.</li>
<li>
<strong>DeepGaze Installation:</strong> The chosen DeepGaze implementation (e.g., DeepGaze III from a specific GitHub repository or package) needs to be installed into this Python environment along with its dependencies (PyTorch, NumPy, PIL/Pillow, etc.).</li>
</ol><p><strong>B.3. Helper Function: <code>install_deepgaze_env()</code></strong></p>
<p>This R function aims to simplify the setup of a Conda environment with DeepGaze. <em>Users should adapt paths and package names based on the specific DeepGaze version and their system.</em> This example assumes a hypothetical <code>deepgaze_pytorch</code> package available via pip and its dependencies.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#' Install a Conda Environment for DeepGaze</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' This function attempts to create a new Conda environment named 'r_deepgaze_env'</span></span>
<span><span class="co">#' and install necessary Python packages for running a PyTorch-based DeepGaze model.</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' @param envname Character string, the name of the Conda environment to create.</span></span>
<span><span class="co">#'                Defaults to "r_deepgaze_env".</span></span>
<span><span class="co">#' @param python_version Character string, the Python version for the environment.</span></span>
<span><span class="co">#'                       Defaults to "3.9".</span></span>
<span><span class="co">#' @param pytorch_cpu_only Logical, if TRUE, installs the CPU-only version of PyTorch.</span></span>
<span><span class="co">#'                         Defaults to TRUE for wider compatibility.</span></span>
<span><span class="co">#'                         Set to FALSE if a GPU version is desired and CUDA is configured.</span></span>
<span><span class="co">#' @param deepgaze_package_name Character string, the pip installable name for DeepGaze.</span></span>
<span><span class="co">#'                              This is HYPOTHETICAL and needs to be replaced with the actual</span></span>
<span><span class="co">#'                              package name or installation command (e.g., from a GitHub repo).</span></span>
<span><span class="co">#'                              Defaults to "deepgaze_pytorch".</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' @details</span></span>
<span><span class="co">#' Users might need to adjust 'deepgaze_package_name' and potentially other</span></span>
<span><span class="co">#' dependencies based on the specific DeepGaze implementation they intend to use.</span></span>
<span><span class="co">#' This function requires Conda to be installed and accessible in the system PATH.</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' It's often more robust to manually create the Python environment and install</span></span>
<span><span class="co">#' DeepGaze following its official documentation, then point reticulate to it.</span></span>
<span><span class="co">#' This function is provided as a convenience but might require troubleshooting.</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' @examples</span></span>
<span><span class="co">#' \dontrun{</span></span>
<span><span class="co">#'   # Before running, ensure 'deepgaze_package_name' is correct for your DeepGaze source</span></span>
<span><span class="co">#'   # install_deepgaze_env(deepgaze_package_name = "pip_package_for_my_deepgaze_version")</span></span>
<span><span class="co">#'   # OR, if installing from GitHub:</span></span>
<span><span class="co">#'   # install_deepgaze_env(deepgaze_package_name = "git+https://github.com/user/deepgaze-repo.git")</span></span>
<span><span class="co">#' }</span></span>
<span><span class="co">#'</span></span>
<span><span class="va">install_deepgaze_env</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">envname</span> <span class="op">=</span> <span class="st">"r_deepgaze_env"</span>,</span>
<span>                                 <span class="va">python_version</span> <span class="op">=</span> <span class="st">"3.9"</span>,</span>
<span>                                 <span class="va">pytorch_cpu_only</span> <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                 <span class="va">deepgaze_package_name</span> <span class="op">=</span> <span class="st">"deepgaze_pytorch"</span><span class="op">)</span> <span class="op">{</span> <span class="co"># HYPOTHETICAL</span></span>
<span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu">reticulate</span><span class="fu">::</span><span class="fu">conda_is_installed</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Conda is not installed or not found by reticulate."</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Please install Miniconda/Anaconda and ensure it's in your PATH,"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"or use reticulate::install_miniconda()."</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/invisible.html" class="external-link">invisible</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">existing_envs</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/conda-tools.html" class="external-link">conda_list</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">name</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">envname</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="va">existing_envs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Conda environment '"</span>, <span class="va">envname</span>, <span class="st">"' already exists."</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"To use it: reticulate::use_condaenv('"</span>, <span class="va">envname</span>, <span class="st">"', required = TRUE)"</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/invisible.html" class="external-link">invisible</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Attempting to create Conda environment: '"</span>, <span class="va">envname</span>, <span class="st">"'..."</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>    <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/conda-tools.html" class="external-link">conda_create</a></span><span class="op">(</span>envname <span class="op">=</span> <span class="va">envname</span>, python_version <span class="op">=</span> <span class="va">python_version</span><span class="op">)</span></span>
<span></span>
<span>    <span class="va">packages_to_install</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"numpy"</span>, <span class="st">"Pillow"</span><span class="op">)</span> <span class="co"># Basic dependencies</span></span>
<span></span>
<span>    <span class="co"># PyTorch installation command varies</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">pytorch_cpu_only</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># Example for CPU-only PyTorch from PyTorch.org (check for current command)</span></span>
<span>      <span class="co"># This command can be highly specific to OS and PyTorch version.</span></span>
<span>      <span class="co"># Using pip install within conda env for simplicity here.</span></span>
<span>      <span class="co"># A more robust way is to use conda install pytorch torchvision torchaudio cpuonly -c pytorch</span></span>
<span>      <span class="va">packages_to_install</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="co"># GPU PyTorch installation is more complex and system-dependent</span></span>
<span>      <span class="co"># (e.g., "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch")</span></span>
<span>      <span class="co"># For this example, we'll stick to a generic pip install command</span></span>
<span>      <span class="co"># that the user would need to ensure installs the GPU version.</span></span>
<span>      <span class="va">packages_to_install</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">"torch torchvision torchaudio"</span><span class="op">)</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Note: For GPU PyTorch, ensure your CUDA drivers and toolkit are correctly set up."</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="co"># Add the DeepGaze package itself (THIS IS HYPOTHETICAL)</span></span>
<span>    <span class="co"># The user MUST replace 'deepgaze_package_name' with the actual pip install command</span></span>
<span>    <span class="co"># or instructions for their chosen DeepGaze version.</span></span>
<span>    <span class="co"># It might be 'pip install deepgaze3', 'pip install git+https://github.com/user/repo.git', etc.</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/startsWith.html" class="external-link">startsWith</a></span><span class="op">(</span><span class="va">deepgaze_package_name</span>, <span class="st">"git+"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>         <span class="va">packages_to_install</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="va">deepgaze_package_name</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>         <span class="va">packages_to_install</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="va">deepgaze_package_name</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Installing packages into '"</span>, <span class="va">envname</span>, <span class="st">"': "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">packages_to_install</span>, collapse<span class="op">=</span><span class="st">", "</span><span class="op">)</span>, <span class="st">"..."</span><span class="op">)</span></span>
<span>    <span class="co"># Install using pip within the conda environment</span></span>
<span>    <span class="co"># Note: reticulate::conda_install can be tricky with complex packages like PyTorch.</span></span>
<span>    <span class="co"># Using pip directly via conda_run is often more reliable for these.</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">pkg</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/strsplit.html" class="external-link">strsplit</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">" "</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span> <span class="co"># simplistic split</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"torch"</span>, <span class="va">pkg</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"http"</span>, <span class="va">pkg</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span> <span class="co"># handle complex torch install command</span></span>
<span>             <span class="fu">reticulate</span><span class="fu">::</span><span class="fu">conda_run</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"pip install"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/strsplit.html" class="external-link">strsplit</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">" "</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"torch"</span>, <span class="fu"><a href="https://rdrr.io/r/base/strsplit.html" class="external-link">strsplit</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">" "</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/strsplit.html" class="external-link">strsplit</a></span><span class="op">(</span><span class="va">packages_to_install</span>, <span class="st">" "</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">]</span>, collapse<span class="op">=</span><span class="st">" "</span><span class="op">)</span><span class="op">)</span>, envname <span class="op">=</span> <span class="va">envname</span><span class="op">)</span></span>
<span>             <span class="kw">break</span> <span class="co"># Assuming torch command is last</span></span>
<span>        <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"http"</span>, <span class="va">pkg</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"git+"</span>, <span class="va">pkg</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>             <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/conda-tools.html" class="external-link">conda_install</a></span><span class="op">(</span>envname <span class="op">=</span> <span class="va">envname</span>, packages <span class="op">=</span> <span class="va">pkg</span>, pip <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>        <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"git+"</span>, <span class="va">pkg</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>             <span class="fu">reticulate</span><span class="fu">::</span><span class="fu">conda_run</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"pip install"</span>, <span class="va">pkg</span><span class="op">)</span>, envname <span class="op">=</span> <span class="va">envname</span><span class="op">)</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Conda environment '"</span>, <span class="va">envname</span>, <span class="st">"' created and packages installed (hopefully!)."</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"To use this environment in your R session, run:"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"reticulate::use_condaenv('"</span>, <span class="va">envname</span>, <span class="st">"', required = TRUE)"</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Then import the DeepGaze module using reticulate::import('deepgaze_module_name')."</span><span class="op">)</span></span>
<span></span>
<span>  <span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Error creating Conda environment '"</span>, <span class="va">envname</span>, <span class="st">"':"</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Please try creating the environment and installing packages manually."</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/invisible.html" class="external-link">invisible</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><strong>Important Considerations for <code>install_deepgaze_env()</code>:</strong> * <strong>Hypothetical Package:</strong> <code>deepgaze_pytorch</code> is a placeholder. The actual installation command for DeepGaze (e.g., from PyPI or GitHub) must be used. * <strong>PyTorch Installation:</strong> PyTorch installation can be tricky and platform-dependent, especially for GPU versions. It’s often best to follow official PyTorch installation instructions for Conda directly. The function provides a basic CPU-only pip attempt. * <strong>Manual Setup Recommended:</strong> For complex dependencies, manually setting up the Conda environment as per the DeepGaze model’s documentation and then pointing <code>reticulate</code> to it (using <code><a href="https://rstudio.github.io/reticulate/reference/use_python.html" class="external-link">reticulate::use_condaenv()</a></code>) is often more reliable.</p>
<p><strong>B.4. Wrapping DeepGaze for Use in R</strong></p>
<p>Once the Python environment is set up and DeepGaze is installed, you can use <code>reticulate</code> to call it from R.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 0. Ensure the correct Conda environment is activated for reticulate</span></span>
<span><span class="co"># This should be done ONCE per R session, ideally at the beginning.</span></span>
<span><span class="co"># Replace 'r_deepgaze_env' with the actual name of your Conda environment.</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>  <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/use_python.html" class="external-link">use_condaenv</a></span><span class="op">(</span><span class="st">"r_deepgaze_env"</span>, required <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Successfully using Conda environment: r_deepgaze_env"</span><span class="op">)</span></span>
<span><span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Could not activate Conda environment 'r_deepgaze_env'."</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Ensure it's created, DeepGaze is installed in it, and the name is correct."</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"You might need to run install_deepgaze_env() or set it up manually."</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># 1. Import the necessary Python modules</span></span>
<span><span class="co"># The exact module name for DeepGaze will depend on its Python implementation.</span></span>
<span><span class="co"># This is HYPOTHETICAL.</span></span>
<span><span class="co"># For example, if DeepGaze III is in a module called 'deepgaze.deepgaze_pytorch':</span></span>
<span><span class="va">deepgaze_module</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>  <span class="co"># This name 'deepgaze_models' or similar is HYPOTHETICAL</span></span>
<span>  <span class="co"># You need to find the actual importable module name from your DeepGaze installation</span></span>
<span>  <span class="va">deepgaze_module</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"deepgaze.pytorch_model_for_example"</span>, delay_load <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="co"># Replace with actual module</span></span>
<span>  <span class="co"># Or, if it's a specific class within a module:</span></span>
<span>  <span class="co"># DeepGazeModel &lt;- reticulate::import_main()$MyDeepGazeClass # If in __main__ after a script run</span></span>
<span>  <span class="co"># Or:</span></span>
<span>  <span class="co"># dg_models &lt;- reticulate::import("some_deepgaze_library.models")</span></span>
<span>  <span class="co"># DeepGazeModel &lt;- dg_models$DeepGazeIII</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"DeepGaze Python module imported (or load delayed)."</span><span class="op">)</span></span>
<span><span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Failed to import the DeepGaze Python module."</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Ensure the module name is correct and it's installed in the active Conda environment."</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 2. Load the pre-trained DeepGaze model (Python side)</span></span>
<span><span class="co"># This step is highly dependent on the specific DeepGaze API.</span></span>
<span><span class="co"># It might involve specifying model paths, device (CPU/GPU), etc.</span></span>
<span><span class="va">deepgaze_model_instance</span> <span class="op">&lt;-</span> <span class="cn">NULL</span> <span class="co"># Global variable to hold the model instance</span></span>
<span></span>
<span><span class="co"># Helper function to initialize the model (Python side) ONCE</span></span>
<span><span class="va">initialize_deepgaze_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">model_weights_path</span> <span class="op">=</span> <span class="st">"path/to/deepgaze_weights.pth"</span>,</span>
<span>                                      <span class="va">device</span> <span class="op">=</span> <span class="st">"cpu"</span><span class="op">)</span> <span class="op">{</span> <span class="co"># "cuda" for GPU</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">deepgaze_model_instance</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">inherits</a></span><span class="op">(</span><span class="va">deepgaze_model_instance</span>, <span class="st">"python.builtin.object"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"DeepGaze model already initialized."</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">deepgaze_model_instance</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Initializing DeepGaze model instance..."</span><span class="op">)</span></span>
<span>  <span class="co"># This is HYPOTHETICAL - adapt to your DeepGaze model's API</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>    <span class="co"># Example: Assuming the imported module has a function like `load_model`</span></span>
<span>    <span class="co"># or a class constructor.</span></span>
<span>    <span class="co"># deepgaze_model_instance &lt;&lt;- deepgaze_module$load_pretrained_model(weights_path = model_weights_path, device = device)</span></span>
<span>    <span class="co"># OR for a class:</span></span>
<span>    <span class="co"># deepgaze_model_instance &lt;&lt;- deepgaze_module$DeepGazeIII(pretrained=TRUE, device=device) # Hypothetical</span></span>
<span>    </span>
<span>    <span class="co"># --- REPLACE THIS BLOCK WITH ACTUAL DEEPGAZE MODEL LOADING CODE ---</span></span>
<span>    <span class="co"># For demonstration, let's assume a placeholder function exists in the Python module</span></span>
<span>    <span class="co"># You would replace 'PlaceholderDeepGazeModel' and its arguments.</span></span>
<span>    <span class="co"># Ensure your Python 'deepgaze.pytorch_model_for_example' has this.</span></span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="st">"PlaceholderDeepGazeModel"</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">deepgaze_module</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">deepgaze_model_instance</span> <span class="op">&lt;&lt;-</span> <span class="va">deepgaze_module</span><span class="op">$</span><span class="fu">PlaceholderDeepGazeModel</span><span class="op">(</span>device <span class="op">=</span> <span class="va">device</span><span class="op">)</span></span>
<span>        <span class="va">deepgaze_model_instance</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span><span class="op">)</span> <span class="co"># Typically set model to evaluation mode</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"DeepGaze model instance initialized and set to eval mode."</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/warning.html" class="external-link">warning</a></span><span class="op">(</span><span class="st">"PlaceholderDeepGazeModel not found in the imported module. Model not loaded."</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"You MUST replace the model loading logic with your actual DeepGaze API."</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="co"># --- END OF REPLACEABLE BLOCK ---</span></span>
<span></span>
<span>  <span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Error initializing DeepGaze model instance in Python:"</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">deepgaze_model_instance</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span></span>
<span><span class="co"># 3. R function to get saliency map for an image</span></span>
<span><span class="co">#' Get Saliency Map using DeepGaze</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' Loads an image, preprocesses it, gets saliency prediction from DeepGaze,</span></span>
<span><span class="co">#' and returns the saliency map as an R matrix or imager::cimg object.</span></span>
<span><span class="co">#'</span></span>
<span><span class="co">#' @param image_path Character string, path to the input image file.</span></span>
<span><span class="co">#' @param model_instance A Python object representing the loaded DeepGaze model.</span></span>
<span><span class="co">#' @param target_size Numeric vector c(width, height), e.g., c(1024, 768),</span></span>
<span><span class="co">#'                    to resize the saliency map to match FDMs.</span></span>
<span><span class="co">#'                    If NULL, original saliency map size is returned.</span></span>
<span><span class="co">#' @return A matrix or imager::cimg object representing the saliency map,</span></span>
<span><span class="co">#'         normalized to sum to 1. Returns NULL on error.</span></span>
<span><span class="co">#'</span></span>
<span><span class="va">get_saliency_deepgaze</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">image_path</span>, <span class="va">model_instance</span>, <span class="va">target_size</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">model_instance</span><span class="op">)</span> <span class="op">||</span> <span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">inherits</a></span><span class="op">(</span><span class="va">model_instance</span>, <span class="st">"python.builtin.object"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"DeepGaze model instance is not valid. Initialize it first."</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/files.html" class="external-link">file.exists</a></span><span class="op">(</span><span class="va">image_path</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"Image file not found: "</span>, <span class="va">image_path</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">PIL</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"PIL.Image"</span>, delay_load <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="va">transforms</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"torchvision.transforms"</span>, delay_load <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="co"># If needed for preprocessing</span></span>
<span>  <span class="va">torch</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"torch"</span>, delay_load <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="co"># If manual tensor ops needed</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>    <span class="co"># Load image using PIL (Python Imaging Library)</span></span>
<span>    <span class="va">img_pil</span> <span class="op">&lt;-</span> <span class="va">PIL</span><span class="op">$</span><span class="fu">open</span><span class="op">(</span><span class="va">image_path</span><span class="op">)</span><span class="op">$</span><span class="fu">convert</span><span class="op">(</span><span class="st">'RGB'</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># --- REPLACE THIS BLOCK WITH ACTUAL DEEPGAZE PREPROCESSING AND PREDICTION ---</span></span>
<span>    <span class="co"># Preprocessing: This is highly model-specific.</span></span>
<span>    <span class="co"># Example: typical PyTorch preprocessing</span></span>
<span>    <span class="co"># preprocess &lt;- transforms$Compose(list(</span></span>
<span>    <span class="co">#   transforms$Resize(list(224L, 224L)), # Example size</span></span>
<span>    <span class="co">#   transforms$ToTensor(),</span></span>
<span>    <span class="co">#   transforms$Normalize(mean=c(0.485, 0.456, 0.406), std=c(0.229, 0.224, 0.225))</span></span>
<span>    <span class="co"># ))</span></span>
<span>    <span class="co"># img_tensor &lt;- preprocess(img_pil)$unsqueeze(0L) # Add batch dimension</span></span>
<span></span>
<span>    <span class="co"># If your model takes a raw PIL image or numpy array, adapt accordingly.</span></span>
<span>    <span class="co"># For our placeholder, let's assume it takes a PIL image and returns a tensor.</span></span>
<span>    <span class="co"># This part is CRITICALLY dependent on your specific DeepGaze model's API.</span></span>
<span>    </span>
<span>    <span class="co"># Get prediction (output is usually a PyTorch tensor)</span></span>
<span>    <span class="co"># saliency_tensor_pred &lt;- model_instance$predict(img_tensor) # Hypothetical predict method</span></span>
<span>    </span>
<span>    <span class="co"># Using our placeholder method that might exist in the Python module</span></span>
<span>    <span class="co"># Ensure your Python 'deepgaze.pytorch_model_for_example' has this.</span></span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="st">"predict_saliency_from_pil"</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">model_instance</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">saliency_tensor_pred</span> <span class="op">&lt;-</span> <span class="va">model_instance</span><span class="op">$</span><span class="fu">predict_saliency_from_pil</span><span class="op">(</span><span class="va">img_pil</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/warning.html" class="external-link">warning</a></span><span class="op">(</span><span class="st">"predict_saliency_from_pil method not found on model instance. Cannot predict."</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="st">"You MUST replace prediction logic with your actual DeepGaze API."</span><span class="op">)</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="co"># --- END OF REPLACEABLE BLOCK ---</span></span>
<span></span>
<span></span>
<span>    <span class="co"># Postprocess: Convert tensor to R matrix, resize, normalize</span></span>
<span>    <span class="co"># Squeeze batch and channel dimensions if necessary (common for saliency maps)</span></span>
<span>    <span class="va">saliency_map_py</span> <span class="op">&lt;-</span> <span class="va">saliency_tensor_pred</span><span class="op">$</span><span class="fu">squeeze</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">cpu</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="kw">detach</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="co"># To NumPy array</span></span>
<span></span>
<span>    <span class="co"># Convert to R matrix</span></span>
<span>    <span class="va">saliency_map_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">saliency_map_py</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Resize if target_size is provided (using imager for convenience)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">target_size</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="fu"><a href="https://rdrr.io/r/base/ns-load.html" class="external-link">requireNamespace</a></span><span class="op">(</span><span class="st">"imager"</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">saliency_cimg</span> <span class="op">&lt;-</span> <span class="fu">imager</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/imager/man/as.cimg.html" class="external-link">as.cimg</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span></span>
<span>      <span class="co"># imager's resize uses x,y,z,c dimensions. Saliency is usually 2D.</span></span>
<span>      <span class="co"># Ensure aspect ratio is handled as desired (e.g., stretch vs. pad)</span></span>
<span>      <span class="va">saliency_cimg_resized</span> <span class="op">&lt;-</span> <span class="fu">imager</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/imager/man/resize.html" class="external-link">resize</a></span><span class="op">(</span><span class="va">saliency_cimg</span>,</span>
<span>                                              size_x <span class="op">=</span> <span class="va">target_size</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>                                              size_y <span class="op">=</span> <span class="va">target_size</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>                                              interpolation_type <span class="op">=</span> <span class="fl">6</span><span class="op">)</span> <span class="co"># Lanczos for quality</span></span>
<span>      <span class="va">saliency_map_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">saliency_cimg_resized</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="co"># Normalize to sum to 1</span></span>
<span>    <span class="va">saliency_map_r</span> <span class="op">&lt;-</span> <span class="va">saliency_map_r</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span> <span class="co"># Ensure non-negative</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1e-6</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">saliency_map_r</span> <span class="op">&lt;-</span> <span class="va">saliency_map_r</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="co"># Handle case of all-zero map (e.g., return uniform distribution)</span></span>
<span>      <span class="va">saliency_map_r</span><span class="op">[</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span><span class="op">)</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/warning.html" class="external-link">warning</a></span><span class="op">(</span><span class="st">"Saliency map was near zero; returning uniform distribution."</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">saliency_map_r</span><span class="op">)</span></span>
<span></span>
<span>  <span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Error getting saliency map for"</span>, <span class="va">image_path</span>, <span class="st">":"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/message.html" class="external-link">message</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># --- Example Usage (Illustrative - requires actual DeepGaze setup) ---</span></span>
<span><span class="co">#</span></span>
<span><span class="co"># # A. One-time setup (or at start of script/project):</span></span>
<span><span class="co"># # 1. Ensure you have a Conda env named "r_deepgaze_env" with Python, PyTorch,</span></span>
<span><span class="co"># #    Pillow, NumPy, and your specific DeepGaze package installed.</span></span>
<span><span class="co"># #    You might use install_deepgaze_env() CAREFULLY or set it up manually.</span></span>
<span><span class="co"># #</span></span>
<span><span class="co"># # install_deepgaze_env(deepgaze_package_name = "actual_pip_name_or_git_url_for_deepgaze")</span></span>
<span><span class="co">#</span></span>
<span><span class="co"># # 2. Activate the environment (already done at the top of this script section)</span></span>
<span><span class="co"># # reticulate::use_condaenv("r_deepgaze_env", required = TRUE)</span></span>
<span><span class="co">#</span></span>
<span><span class="co"># # 3. Initialize the DeepGaze model (Python side)</span></span>
<span><span class="co"># #    Replace "path/to/deepgaze_weights.pth" with the actual path if your model</span></span>
<span><span class="co"># #    requires loading weights explicitly. Some models load them automatically if pretrained.</span></span>
<span><span class="co"># #    The Python module `deepgaze.pytorch_model_for_example` and the class/function</span></span>
<span><span class="co"># #    `PlaceholderDeepGazeModel` and its method `predict_saliency_from_pil`</span></span>
<span><span class="co"># #    ARE HYPOTHETICAL and need to be replaced with your actual DeepGaze model's API.</span></span>
<span><span class="co">#</span></span>
<span><span class="co"># # dg_model &lt;- initialize_deepgaze_model(device = "cpu") # or "cuda"</span></span>
<span><span class="co">#</span></span>
<span><span class="co"># # B. For each image:</span></span>
<span><span class="co"># # if (!is.null(dg_model)) {</span></span>
<span><span class="co"># #   # Define target dimensions for the saliency map, e.g., matching your screen/FDM resolution</span></span>
<span><span class="co"># #   screen_width &lt;- 1024</span></span>
<span><span class="co"># #   screen_height &lt;- 768</span></span>
<span><span class="co"># #   saliency_map &lt;- get_saliency_deepgaze(</span></span>
<span><span class="co"># #     image_path = "path/to/your/stimulus_image.jpg",</span></span>
<span><span class="co"># #     model_instance = dg_model,</span></span>
<span><span class="co"># #     target_size = c(screen_width, screen_height)</span></span>
<span><span class="co"># #   )</span></span>
<span><span class="co"># #</span></span>
<span><span class="co"># #   if (!is.null(saliency_map)) {</span></span>
<span><span class="co"># #     # Now 'saliency_map' is an R matrix, normalized, and ready for use</span></span>
<span><span class="co"># #     # in the Signed EMD calculation (see Addendum A)</span></span>
<span><span class="co"># #     print(paste("Saliency map dimensions:", paste(dim(saliency_map), collapse="x")))</span></span>
<span><span class="co"># #     print(paste("Sum of saliency map:", sum(saliency_map))) # Should be 1</span></span>
<span><span class="co"># #</span></span>
<span><span class="co"># #     if (requireNamespace("imager", quietly = TRUE) &amp;&amp; interactive()) {</span></span>
<span><span class="co"># #        # Quick plot if imager is available</span></span>
<span><span class="co"># #        # plot(imager::as.cimg(saliency_map), main="DeepGaze Saliency Map")</span></span>
<span><span class="co"># #     }</span></span>
<span><span class="co"># #   }</span></span>
<span><span class="co"># # } else {</span></span>
<span><span class="co"># #   message("DeepGaze model not initialized. Cannot generate saliency map.")</span></span>
<span><span class="co"># # }</span></span></code></pre></div>
<p><strong>B.5. Crucial Considerations and Customization:</strong></p>
<ul><li>
<strong>DeepGaze API Specifics:</strong> The core of <code>initialize_deepgaze_model</code> and <code>get_saliency_deepgaze</code> (especially image preprocessing, model prediction call, and output handling) <strong>MUST be adapted</strong> to the precise API of the DeepGaze version being used. The provided code uses placeholders (e.g., <code>deepgaze.pytorch_model_for_example</code>, <code>PlaceholderDeepGazeModel</code>, <code>predict_saliency_from_pil</code>). These will not work out-of-the-box and require replacement with actual, functional Python code corresponding to the chosen DeepGaze library.</li>
<li>
<strong>Python Module Name:</strong> The <code>reticulate::import("module_name")</code> call needs the correct Python module name where the DeepGaze model is defined.</li>
<li>
<strong>Model Loading:</strong> How pre-trained weights are loaded is model-specific. Some models load them automatically with <code>pretrained=True</code>, others require a path to a weights file.</li>
<li>
<strong>Preprocessing:</strong> Image preprocessing steps (resizing, normalization, tensor conversion) must match what the DeepGaze model expects during its training.</li>
<li>
<strong>Output Format:</strong> Saliency models output data in various formats (e.g., PyTorch tensors, NumPy arrays). The R code needs to handle this and convert it to a suitable R matrix.</li>
<li>
<strong>Device Management (CPU/GPU):</strong> If using a GPU, ensure PyTorch (or TensorFlow) is installed with GPU support in the Conda environment and that <code>device="cuda"</code> (or similar) is correctly passed to the model and tensors.</li>
<li>
<strong>Error Handling:</strong> Robust error handling is important, especially around Python interactions.</li>
<li>
<strong>Efficiency:</strong> For processing many images, initializing the Python model once and reusing the instance (<code>deepgaze_model_instance</code>) is crucial for performance, as model loading can be slow.</li>
</ul><p>This addendum provides a structural template. Successful integration will require careful attention to the documentation and API of the specific DeepGaze implementation chosen for the project. It’s advisable to first ensure the DeepGaze model runs correctly in a pure Python script within the target Conda environment before attempting to wrap it with <code>reticulate</code>.</p>
</div>


  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Bradley Buchsbaum.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

