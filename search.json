[{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"project-overview","dir":"","previous_headings":"","what":"Project Overview","title":"CLAUDE.md","text":"eyesim R package analyzing eye-movement data, focusing computing fixation pattern similarity, density map generation, scanpath analysis, recognition memory studies. package implements various eye-tracking analysis methods including MultiMatch algorithm, CRQA, EMD-based similarity measures.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"building-and-testing","dir":"","previous_headings":"Common Development Commands","what":"Building and Testing","title":"CLAUDE.md","text":"","code":"# Install dependencies devtools::install_deps()  # Run tests devtools::test() # Run a single test file testthat::test_file(\"tests/testthat/test_similarity_emd.R\")  # Check package (includes running tests) devtools::check()  # Generate documentation from roxygen2 comments devtools::document()  # Build package devtools::build()  # Install locally for testing devtools::install()"},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"documentation","dir":"","previous_headings":"Common Development Commands","what":"Documentation","title":"CLAUDE.md","text":"","code":"# Build pkgdown documentation site pkgdown::build_site()  # Check test coverage covr::package_coverage()"},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"core-data-structures","dir":"","previous_headings":"Architecture","what":"Core Data Structures","title":"CLAUDE.md","text":"Contains fixations x, y coordinates, onset, duration, subject info Supports grouping trial, subject, condition Grid-based representation fixation patterns Supports various density estimation methods fixation_group: Groups fixations metadata (R/fixations.R)","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"key-modules","dir":"","previous_headings":"Architecture","what":"Key Modules","title":"CLAUDE.md","text":"Template-based similarity EMD (Earth Mover’s Distance) based similarity Spatial temporal similarity measures Computes 5 similarity dimensions: vector, direction, length, position, duration density_by() grouped density estimation multiscale_density() multi-resolution analysis CRQA (R/crqa.R): Cross-recurrence quantification analysis scanpath dynamics","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"testing-approach","dir":"","previous_headings":"Architecture","what":"Testing Approach","title":"CLAUDE.md","text":"Tests use testthat framework fixtures tests/testthat/. major function corresponding test file. Tests focus : - Correct computation similarity metrics - Proper handling edge cases (empty data, single fixations) - Consistency density estimation methods - Proper S3 method dispatch","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/CLAUDE.html","id":"package-dependencies","dir":"","previous_headings":"Architecture","what":"Package Dependencies","title":"CLAUDE.md","text":"Key dependencies include: - proxy: Distance matrix computation - imager: Image processing density maps - ggplot2: Visualization - dplyr/tidyr: Data manipulation - Matrix: Sparse matrix operations efficiency","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/advanced_proposal.html","id":"revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies","dir":"","previous_headings":"","what":"Revised Proposal: Advancing Eye-Movement Similarity Analysis for Encoding-Retrieval Studies","title":"NA","text":"1. Introduction & Motivation Current methods quantifying eye-movement similarity encoding retrieval (e.g., cosine similarity single-scale Fixation Density Maps - FDMs) offer valuable insights may lack sensitivity capture full richness gaze reinstatement. proposal outlines staged approach implement rigorously evaluate increasingly powerful techniques measuring eye-movement similarity. goal enhance sensitivity, improve interpretability possible, establish data-driven framework selecting optimal metric(s) understanding gaze-based memory reinstatement. pay close attention stimulus similarity foils, evaluating metric performance just identical image pairs also -category, different-instance pictures ensure measures capture specific memory reinstatement rather generic saliency overlap. 1.1. Gaze-Data Quality Control Prior similarity analysis, rigorous data quality control implemented: * Track-Loss Handling: Frames >[X]% track loss within [Y]ms window (equivalent pupil-size validity metrics) identified. Short periods track loss (<[Z]ms) may interpolated appropriate analysis stage; longer periods lead fixation exclusion trial rejection. * Exclusion Thresholds: Trials <[N] valid fixations total gaze data [P]% trial duration excluded. Participants >[Q]% trial exclusion considered removal analysis. * Calibration: Re-calibration procedures performed encoding retrieval blocks (fixed intervals) maintain high spatial accuracy. Drift correction applied necessary. 2. Overarching Strategy & Guiding Principles approach built following principles: Incremental Complexity: progress simpler, drop-upgrades sophisticated, novel methods, ensuring step justifies added complexity. Quantitative Gating: Pre-defined statistical thresholds improvement guide decisions proceed complex stages. Interpretability Balance: seeking maximum sensitivity, prioritize interpretable methods include strategies “peek inside” black-box approaches. Rigorous Validation: comprehensive validation framework, including noise ceiling estimation qualitative error analysis, applied consistently. Participant-Level Sensitivity: explore account individual differences optimal metric parameters. Task Variant Consideration: proposal assumes [e.g., full-image re-presentation retrieval]. multiple task variants used (e.g., full-image vs. partial-cue delay), metrics evaluated separately . Metrics pooled across tasks pilot analysis demonstrates metric × task interactions negligible primary outcomes. 3. Methodological Stages & Evaluation (graphical concept figure developed visually illustrate scanpaths representation stage: FDM, multi-scale FDM, DTW path, graph representation. appear Figure 1 final methods.) Stage 0: Establish Current Baseline * (Effort ≈ 1 week; RAM ≈ 4 GB) * Method: Re-implement current standard: Fixation Density Map (FDM) smoothed single σ, cosine similarity. * Evaluation: Reproduce published/internal effect sizes key behavioural correlates (e.g., recognition accuracy, vividness). serves primary benchmark. Stage 1: Upgrade Heat-Map Metric (Representation, Better Distance) * (Effort ≈ 2 weeks; RAM ≈ 8 GB) * Improvements: 1. Earth-Mover’s Distance (EMD/Wasserstein-1): Replace cosine similarity EMD FDMs. Optionally, sign EMD subtracting per-image saliency map (e.g., using pre-trained model like DeepGaze III, ITTI, similar, specified priori). Scene-dependent failures saliency model monitored qualitative error-triage. * Tool: transport::wasserstein() R. 2. Multi-Scale Similarity: Compute FDMs multiple spatial scales (e.g., σ = 0.25°, 0.5°, 1.0°) average (1 – EMD) similarity across scales. 3. Duration-Weighted Density: Weight fixations duration (inverse saccade velocity) creating density maps. * Evaluation: * Compare multi-scale, duration-weighted EMD Stage 0 using permutation z-scores ΔAUC (macro) -image classification. * Quantitative Gate: Proceed ΔAUC ≥ max(0.02, 0.05 * baseline_AUC_standard_error) 95% CI ΔAUC > 0. * power analysis (via simulation) pre-registered ensure adequate power (e.g., 80%) detect ΔAUC gate expected number participant-item pairs. * Interpretability: EMD remains relatively interpretable. Stage 2: Bring Back Time – Soft Sequence Alignment * (Effort ≈ 2-3 weeks; RAM ≈ 8-16 GB) * Improvements: 1. Soft Dynamic Time Warping (soft-DTW): Apply soft-DTW raw (x,y) fixation sequences. DTW cost function (e.g., Euclidean distance) fixed priori. * Tool: dtwclust tsclust R. 2. (Alternative) Temporal Cross-Recurrence Plot (CRP): Summarise diagonality binary matrix pairwise distances < ε encoding retrieval fixations. bandwidth ε fixed priori. 3. Combined Metric: Fuse spatial metric Stage 1 temporal metric (soft-DTW distance) using simple weighted sum. * Evaluation: * Compare Stage 1 + soft-DTW fusion Stage 1 alone. Assess impact behavioural correlations using mixed-effects models. * Quantitative Gate: Proceed Δβ (standardized, vividness prediction) ≥ +0.10 p < 0.01 (Holm-corrected) interaction term main effect improved metric. * Interpretability: DTW alignment paths CRPs offer visual insights. Stage 3: Graph-Theoretic Representation (Novel & Interpretable Features) * (Effort ≈ 1 month; RAM ≈ 30-40 GB kernels ~10k pairs; HPC/GPU access planned needed) * Representation: Convert scanpath undirected weighted graph (Nodes = Fixation centroids; Edge weight = Saccade probability). * Similarity Metrics/Features: 1. Global Similarity (Graph Edit Distance, Delta-Con). 2. Graph Kernels (e.g., Weisfeiler-Lehman) fed SVM. 3. Extracted Interpretable Features (Degree centrality Gini, Edge-betweenness AOIs, Triad census). * Tool: igraph R, graphkernels via reticulate. * Evaluation: * Compare model using EMD + DTW + extracted graph features Stage 2. * Test using 10-fold nested CV -image classification. * Demonstrate unique predictive variance using hierarchical variance-partitioning (LRT), cross-validated ΔAUC decomposition, collinearity checks (VIF < 3), feature importance. * Quantitative Gate: Proceed Stage 4 Δχ² (LRT graph features) shows p < 0.005. * Interpretability: Graph features directly interpretable; kernels can probed. Stage 4: Contrastive Metric Learning Siamese ScanPath Encoder (Powerful & New) * (Effort ≈ 1-2 months; RAM ≈ 30-40 GB training ~10k pairs; HPC/GPU access planned) * Representation: Sequence fixations (x, y, duration, velocity) → positional embeddings → small Transformer 1D CNN. * Loss Function: InfoNCE / Triplet loss. * Output: Learned embedding space cosine similarity reflects scanpath similarity. * Tool: torch keras R. * Decision Rule & Data Requirements: “Siamese encoder proceeds Stage 3 < 85 % picture-ID ceiling (see Section 4.1) sample size ≥ 3,000 positive pairs.” Heavy data augmentation used. * Evaluation: Compare learned similarity Stage 3. * Interpretability Probes: t-SNE plots, Gradient × Input, frozen backbone + GLM. Stage 5: Hybrid “Gaze-Conditioned Optimal Transport” (Blue-Sky Idea) * (Effort ≈ 1 month (pursued); RAM dependent implementation, likely >16GB) * Representation: Fixation 5-D point: (x, y, t, Δt, v). * Similarity: 5-D Wasserstein distance optimized λ_t, λ_v per participant. * Evaluation: Ablation tests (λ_v=0 vs. optimized), variance-partitioning. 4. Cross-Cutting Validation, Power Boost, Decision Framework 4.1. Estimating “Noise Ceiling” / Classification Head-Room inform 85% decision rule Stage 4: Split-Half Scanpath Reliability: previously described. Picture-ID Oracle: (Leave-picture-CV). Subject-ID Oracle: (Leave-subject-CV). Joint Oracle: (Random 80/20 split). three reported. “effective” ceiling cross-subject generalization considered lower picture-ID oracle GBT proxy avoid inflation subject fingerprints. Bayes Error Proxy (GBT): XGBoost flattened fixation sequences (first 30 enc + 30 ret, NA padding, plus fixation counts). Rule Update Stage 4: multi-scale-EMD + soft-DTW (Stage 2) graph-based model (Stage 3) performance compared 85% Picture-ID Oracle ceiling (GBT proxy lower deemed appropriate generalization). 4.2. General Validation Tools Permutation Framework: previously described (z-scores). Multi-Metric Fusion: “Multi-metric fusion explored single-metric benchmarking avoid curse--dimensionality.” pursued, logistic regression random forest used. Reliability Check: previously described (split-half). Net Re-classification Improvement (NRI): Pre-registered secondary criterion model comparison, guarding cases rank order improves (AUC) calibration worsens. 4.3. Systematic Participant-Level Optimization & Heterogeneity “Participant-level optimisation: subject-specific σ/ω (relevant parameters like λ Stage 5) become random slopes mixed-effects models; inference reported group level partial pooling (e.g., via hierarchical Bayesian models tailoring shows significant benefit via LRT comparison random slope vs. fixed slope models).” 1. Per-subject hyper-parameter grid optimization. 2. Random-slopes mixed models. 3. Report heterogeneity optimal parameters correlate traits. 4. Shrinkage estimators (hierarchical Bayesian models) tailoring helps. 4.4. Qualitative Error Triage (Embedded Stage Ends) systematic qualitative audit “difficult” trials: 1. Identify discrepant pairs (top false positives/negatives; high-vividness/low-similarity quartiles). 2. Visual diagnostics (dual heat-maps, animated replays, AOI strings). 3. Annotation worksheet two coders (tagging factors like partial cues, central bias, idiosyncratic strategy). 4. Feedback loop inform metric tweaks. tiny Shiny dashboard developed early (heat-map overlay, GIF replay, coding form) streamline process reduce coder burden; tool mentioned Methods. “Qualitative Error Analysis” subsection feature representative vignettes. 5. Computational Logistics & Reproducibility Tool Chain: R (transport, dtwclust, igraph, targets workflow management), Python via reticulate (graphkernels), torch/keras (R Python). Hardware Budget: Explicit RAM potential HPC/GPU needs noted per stage. Data Storage & Pre-computation: Large O(n²) similarity matrices managed, potentially HDF5 fst efficient caching. Data Versioning: data-versioning plan (e.g., DVC - Data Version Control, Git-LFS key matrices/CSVs) implemented ensure pre-computed similarity matrices intermediate datasets traceable shareable. Reproducibility: minimal Docker/Singularity recipe provided Appendix facilitate turnkey reproducibility computational environment. 6. Implementation Roadmap & Quantitative Gates 7. Conclusion & Expected Outcomes systematic, multi-stage proposal provides clear path significantly enhance analysis eye-movement similarity. combining incremental advancements rigorous, quantitatively-gated evaluations, qualitative insights, commitment open science, aim : Develop sensitive nuanced measures gaze reinstatement. Understand contributions spatial, temporal, dynamic gaze features. Identify complex models offer benefits simpler ones. Provide robust framework adaptable future research. Dissemination Plan: commit open science practices. include: * Pre-registration study plan (e.g., OSF). * Making analysis code open-source (e.g., via GitHub). * Sharing anonymized data pre-computed similarity matrices ethically permissible feasible. * Publishing findings peer-reviewed journals presenting conferences, preprint made available (e.g., bioRxiv/PsyArXiv). Okay, add explanation addendum main proposal.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/advanced_proposal.html","id":"revised-proposal-advancing-eye-movement-similarity-analysis-for-encoding-retrieval-studies-1","dir":"","previous_headings":"","what":"Revised Proposal: Advancing Eye-Movement Similarity Analysis for Encoding-Retrieval Studies","title":"NA","text":"(Main proposal content previously detailed remains – Section 1. Introduction & Motivation Section 7. Conclusion & Expected Outcomes) Addendum: Explanation “Signing” Earth-Mover’s Distance (EMD) addendum clarifies concept implementation “signing” Earth-Mover’s Distance, technique optionally employed Stage 1 proposed methodology refine eye-movement similarity measures accounting bottom-visual saliency. .1. Plain-English Idea Eye-tracking heat maps naturally -positive probability distributions: every pixel holds non-negative fixation density, whole map sums 1. vanilla Earth-Mover Distance (EMD) two maps always non-negative—tells much work takes morph one distribution . However, suppose observers (e.g., encoding retrieval image) spent lot time regions already highly bottom-salient (e.g., bright areas, high-contrast corners, faces). significant portion measured EMD might simply reflect shared “saliency gravity,” rather idiosyncratic, memory-driven component gaze often primary interest cognitive studies. subtracting saliency baseline map first, fixation heat map converted signed residual map. residual maps: * Positive values indicate regions viewer looked predicted bottom-saliency. * Negative values indicate regions viewer looked less predicted bottom-saliency. Computing EMD residual maps, therefore, measures similarly two scanpaths depart baseline saliency predictions. resulting score can positive negative (hence “signed EMD”), reflecting nature similarity departures. .2. Step--Step Recipe Signed EMD S(x,y) ← Prediction pre-specified saliency model (e.g., DeepGaze III, ITTI & Koch, another chosen priori model), normalized integrate 1. choice saliency model documented, potential scene-dependent failures noted area monitoring qualitative error-triage. F_enc(x,y) (encoding) F_ret(x,y) (retrieval), smoothed kernel (defined Stage 1) normalized integrate 1. R_enc = F_enc - S R_ret = F_ret - S maps now represent deviation observed gaze saliency predictions. sum pixel values residual map zero. R_enc_positive(x,y) = max(R_enc(x,y), 0) R_enc_negative(x,y) = max(-R_enc(x,y), 0) similarly R_ret. R_positive R_negative maps now forms proper, non-negative distribution. total mass (sum pixel values) R_positive equal total mass R_negative given original residual map (positive negative parts residual map R must cancel R sums zero). EMD_positive = EMD R_enc_positive R_ret_positive. EMD_negative = EMD R_enc_negative R_ret_negative. measure “work” required transform “looked saliency” patterns , similarly “looked less saliency” patterns. SignedEMD = -(EMD_positive + EMD_negative) Small (negative) values indicate large total EMD cost, meaning two observers departed saliency dissimilar ways. Large (less negative even positive, approaching zero) values indicate low total EMD cost, meaning two observers -shot -shot saliency similar locations, suggesting genuine reinstatement gaze patterns beyond bottom-pull. (Note: sign convention arbitrary long ’s consistent. researchers might report negative total cost “higher = similar.” chosen convention clearly stated.) .3. Bother Signed EMD? Controls Bottom-Bias: scanpaths align highly salient features simply features attract everyone’s gaze, subtracting saliency map neutralizes shared variance, allowing cleaner measure top-influences. Highlights Mnemonic Task-Driven Guidance: residual gaze patterns (regions looked less predicted saliency) stronger candidates reflecting top-, memory-driven, task-relevant fixation choices. Produces Effect Direction (Potentially): sum two non-negative EMDs always non-negative, specific formulation interpretation focus similarity deviations. term “signed” primarily refers residual maps . final similarity score, defined , range highly negative (dissimilar deviations) towards zero (similar deviations). .4. Quick R Pseudo-Code Example (pp() pseudo-code placeholder; actual conversion input format required transport::wasserstein 2D histograms needed, e.g., ensuring structured outputs hist two-column matrices coordinates masses point patterns.) .5. Take-Home Message “Signing EMD” methodological refinement aims isolate cognitive component gaze similarity. first correcting fixation map baseline, bottom-visual saliency, subsequent EMD calculation reflects similarly two gaze patterns deviate predictions. approach provides targeted measure memory-driven task-relevant gaze reinstatement, rather simply quantifying overlap raw saliency-influenced gaze distributions. resulting signed_similarity_score used within statistical framework similarity metrics. Okay, add second addendum detailing wrap DeepGaze R via reticulate, including helper function installation. assumes Python environment DeepGaze III (similar version) accessible can set . (Main proposal content previously detailed remains – Section 1. Introduction & Motivation Section 7. Conclusion & Expected Outcomes, followed Addendum : Explanation “Signing” Earth-Mover’s Distance (EMD)) Addendum B: Integrating DeepGaze Saliency Model R via Reticulate addendum provides practical guide using Python-based saliency model like DeepGaze III within R workflow using reticulate package. relevant Stage 1 optionally subtracting per-image saliency map compute Signed EMD. B.1. Overview DeepGaze models typically implemented Python using deep learning frameworks like PyTorch TensorFlow. reticulate package R allows seamless interoperability R Python, enabling us call Python functions use Python objects directly R. B.2. Prerequisites Python Installation: working Python installation (e.g., Python 3.7+) required. R Reticulate: R reticulate package must installed (install.packages(\"reticulate\")). Python Environment DeepGaze: ’s highly recommended create dedicated Python virtual environment (e.g., using conda venv) manage DeepGaze dependencies without conflicting Python projects. DeepGaze Installation: chosen DeepGaze implementation (e.g., DeepGaze III specific GitHub repository package) needs installed Python environment along dependencies (PyTorch, NumPy, PIL/Pillow, etc.). B.3. Helper Function: install_deepgaze_env() R function aims simplify setup Conda environment DeepGaze. Users adapt paths package names based specific DeepGaze version system. example assumes hypothetical deepgaze_pytorch package available via pip dependencies. Important Considerations install_deepgaze_env(): * Hypothetical Package: deepgaze_pytorch placeholder. actual installation command DeepGaze (e.g., PyPI GitHub) must used. * PyTorch Installation: PyTorch installation can tricky platform-dependent, especially GPU versions. ’s often best follow official PyTorch installation instructions Conda directly. function provides basic CPU-pip attempt. * Manual Setup Recommended: complex dependencies, manually setting Conda environment per DeepGaze model’s documentation pointing reticulate (using reticulate::use_condaenv()) often reliable. B.4. Wrapping DeepGaze Use R Python environment set DeepGaze installed, can use reticulate call R. B.5. Crucial Considerations Customization: DeepGaze API Specifics: core initialize_deepgaze_model get_saliency_deepgaze (especially image preprocessing, model prediction call, output handling) MUST adapted precise API DeepGaze version used. provided code uses placeholders (e.g., deepgaze.pytorch_model_for_example, PlaceholderDeepGazeModel, predict_saliency_from_pil). work ---box require replacement actual, functional Python code corresponding chosen DeepGaze library. Python Module Name: reticulate::import(\"module_name\") call needs correct Python module name DeepGaze model defined. Model Loading: pre-trained weights loaded model-specific. models load automatically pretrained=True, others require path weights file. Preprocessing: Image preprocessing steps (resizing, normalization, tensor conversion) must match DeepGaze model expects training. Output Format: Saliency models output data various formats (e.g., PyTorch tensors, NumPy arrays). R code needs handle convert suitable R matrix. Device Management (CPU/GPU): using GPU, ensure PyTorch (TensorFlow) installed GPU support Conda environment device=\"cuda\" (similar) correctly passed model tensors. Error Handling: Robust error handling important, especially around Python interactions. Efficiency: processing many images, initializing Python model reusing instance (deepgaze_model_instance) crucial performance, model loading can slow. addendum provides structural template. Successful integration require careful attention documentation API specific DeepGaze implementation chosen project. ’s advisable first ensure DeepGaze model runs correctly pure Python script within target Conda environment attempting wrap reticulate.","code":"# Assume F_enc, F_ret, and S are matrices (e.g., from imager::as.cimg()) # representing fixation density maps and the saliency map, # all appropriately smoothed and normalized to sum to 1.  # library(imager) # for image handling, pmax # library(transport) # for wasserstein()  # 1. Saliency map S is pre-computed # 2. F_enc and F_ret are generated  # 3. Form signed residual maps res_enc <- F_enc - S res_ret <- F_ret - S  # 4. Split residuals into positive and negative \"piles\" pos_enc <- pmax(res_enc, 0) neg_enc <- pmax(-res_enc, 0) # Note: -res_enc, so positive values here represent where original was negative  pos_ret <- pmax(res_ret, 0) neg_ret <- pmax(-res_ret, 0)  # Ensure these are suitable for transport::wasserstein (e.g., as 2D histograms or point patterns) # This might require converting matrix representations to the format expected by the function. # For 2D histograms, transport::wasserstein() expects inputs that are themselves # representations of distributions (e.g., objects from hist() or similar structures). # If F_enc etc. are simple matrices, they might need to be converted. # For simplicity, let's assume they can be directly used or easily converted.  # 5. Compute two EMDs (using p=1 for Wasserstein-1 distance) # The exact call to wasserstein() will depend on how pos_enc etc. are structured. # Assuming they are compatible 2D histograms / distributions: emd_pos <- transport::wasserstein(pp(pos_enc), pp(pos_ret), p = 1) # pp() might be a placeholder for conversion emd_neg <- transport::wasserstein(pp(neg_enc), pp(neg_ret), p = 1) # to point pattern or suitable histogram format  # 6. Derive a single signed similarity score signed_similarity_score <- -(emd_pos + emd_neg) # Higher values (closer to 0) mean more similar deviations  # This signed_similarity_score is then used in subsequent analyses # (permutation framework, mixed models, etc.) in place of the vanilla EMD. #' Install a Conda Environment for DeepGaze #' #' This function attempts to create a new Conda environment named 'r_deepgaze_env' #' and install necessary Python packages for running a PyTorch-based DeepGaze model. #' #' @param envname Character string, the name of the Conda environment to create. #'                Defaults to \"r_deepgaze_env\". #' @param python_version Character string, the Python version for the environment. #'                       Defaults to \"3.9\". #' @param pytorch_cpu_only Logical, if TRUE, installs the CPU-only version of PyTorch. #'                         Defaults to TRUE for wider compatibility. #'                         Set to FALSE if a GPU version is desired and CUDA is configured. #' @param deepgaze_package_name Character string, the pip installable name for DeepGaze. #'                              This is HYPOTHETICAL and needs to be replaced with the actual #'                              package name or installation command (e.g., from a GitHub repo). #'                              Defaults to \"deepgaze_pytorch\". #' #' @details #' Users might need to adjust 'deepgaze_package_name' and potentially other #' dependencies based on the specific DeepGaze implementation they intend to use. #' This function requires Conda to be installed and accessible in the system PATH. #' #' It's often more robust to manually create the Python environment and install #' DeepGaze following its official documentation, then point reticulate to it. #' This function is provided as a convenience but might require troubleshooting. #' #' @examples #' \\dontrun{ #'   # Before running, ensure 'deepgaze_package_name' is correct for your DeepGaze source #'   # install_deepgaze_env(deepgaze_package_name = \"pip_package_for_my_deepgaze_version\") #'   # OR, if installing from GitHub: #'   # install_deepgaze_env(deepgaze_package_name = \"git+https://github.com/user/deepgaze-repo.git\") #' } #' install_deepgaze_env <- function(envname = \"r_deepgaze_env\",                                  python_version = \"3.9\",                                  pytorch_cpu_only = TRUE,                                  deepgaze_package_name = \"deepgaze_pytorch\") { # HYPOTHETICAL    if (!reticulate::conda_is_installed()) {     message(\"Conda is not installed or not found by reticulate.\")     message(\"Please install Miniconda/Anaconda and ensure it's in your PATH,\")     message(\"or use reticulate::install_miniconda().\")     return(invisible(NULL))   }    existing_envs <- reticulate::conda_list()$name   if (envname %in% existing_envs) {     message(paste(\"Conda environment '\", envname, \"' already exists.\", sep = \"\"))     message(\"To use it: reticulate::use_condaenv('\", envname, \"', required = TRUE)\", sep = \"\")     return(invisible(NULL))   }    message(paste(\"Attempting to create Conda environment: '\", envname, \"'...\", sep = \"\"))   tryCatch({     reticulate::conda_create(envname = envname, python_version = python_version)      packages_to_install <- c(\"numpy\", \"Pillow\") # Basic dependencies      # PyTorch installation command varies     if (pytorch_cpu_only) {       # Example for CPU-only PyTorch from PyTorch.org (check for current command)       # This command can be highly specific to OS and PyTorch version.       # Using pip install within conda env for simplicity here.       # A more robust way is to use conda install pytorch torchvision torchaudio cpuonly -c pytorch       packages_to_install <- c(packages_to_install, \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")     } else {       # GPU PyTorch installation is more complex and system-dependent       # (e.g., \"conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\")       # For this example, we'll stick to a generic pip install command       # that the user would need to ensure installs the GPU version.       packages_to_install <- c(packages_to_install, \"torch torchvision torchaudio\")       message(\"Note: For GPU PyTorch, ensure your CUDA drivers and toolkit are correctly set up.\")     }      # Add the DeepGaze package itself (THIS IS HYPOTHETICAL)     # The user MUST replace 'deepgaze_package_name' with the actual pip install command     # or instructions for their chosen DeepGaze version.     # It might be 'pip install deepgaze3', 'pip install git+https://github.com/user/repo.git', etc.     if (startsWith(deepgaze_package_name, \"git+\")) {          packages_to_install <- c(packages_to_install, deepgaze_package_name)     } else {          packages_to_install <- c(packages_to_install, deepgaze_package_name)     }       message(\"Installing packages into '\", envname, \"': \", paste(packages_to_install, collapse=\", \"), \"...\")     # Install using pip within the conda environment     # Note: reticulate::conda_install can be tricky with complex packages like PyTorch.     # Using pip directly via conda_run is often more reliable for these.     for (pkg in strsplit(packages_to_install, \" \")[[1]]) { # simplistic split         if (grepl(\"torch\", pkg) && grepl(\"http\", pkg)) { # handle complex torch install command              reticulate::conda_run(paste(\"pip install\", paste(strsplit(packages_to_install, \" \")[[1]][which(grepl(\"torch\", strsplit(packages_to_install, \" \")[[1]])):length(strsplit(packages_to_install, \" \")[[1]])], collapse=\" \")), envname = envname)              break # Assuming torch command is last         } else if (!grepl(\"http\", pkg) && !grepl(\"git+\", pkg)) {              reticulate::conda_install(envname = envname, packages = pkg, pip = TRUE)         } else if (grepl(\"git+\", pkg)) {              reticulate::conda_run(paste(\"pip install\", pkg), envname = envname)         }     }       message(paste(\"Conda environment '\", envname, \"' created and packages installed (hopefully!).\", sep = \"\"))     message(\"To use this environment in your R session, run:\")     message(\"reticulate::use_condaenv('\", envname, \"', required = TRUE)\", sep = \"\")     message(\"Then import the DeepGaze module using reticulate::import('deepgaze_module_name').\")    }, error = function(e) {     message(paste(\"Error creating Conda environment '\", envname, \"':\", sep = \"\"))     message(e)     message(\"Please try creating the environment and installing packages manually.\")   })   return(invisible(NULL)) } # 0. Ensure the correct Conda environment is activated for reticulate # This should be done ONCE per R session, ideally at the beginning. # Replace 'r_deepgaze_env' with the actual name of your Conda environment. tryCatch({   reticulate::use_condaenv(\"r_deepgaze_env\", required = TRUE)   message(\"Successfully using Conda environment: r_deepgaze_env\") }, error = function(e) {   message(\"Could not activate Conda environment 'r_deepgaze_env'.\")   message(\"Ensure it's created, DeepGaze is installed in it, and the name is correct.\")   message(\"You might need to run install_deepgaze_env() or set it up manually.\")   stop(e) })   # 1. Import the necessary Python modules # The exact module name for DeepGaze will depend on its Python implementation. # This is HYPOTHETICAL. # For example, if DeepGaze III is in a module called 'deepgaze.deepgaze_pytorch': deepgaze_module <- NULL tryCatch({   # This name 'deepgaze_models' or similar is HYPOTHETICAL   # You need to find the actual importable module name from your DeepGaze installation   deepgaze_module <- reticulate::import(\"deepgaze.pytorch_model_for_example\", delay_load = TRUE) # Replace with actual module   # Or, if it's a specific class within a module:   # DeepGazeModel <- reticulate::import_main()$MyDeepGazeClass # If in __main__ after a script run   # Or:   # dg_models <- reticulate::import(\"some_deepgaze_library.models\")   # DeepGazeModel <- dg_models$DeepGazeIII   message(\"DeepGaze Python module imported (or load delayed).\") }, error = function(e) {   message(\"Failed to import the DeepGaze Python module.\")   message(\"Ensure the module name is correct and it's installed in the active Conda environment.\")   stop(e) })  # 2. Load the pre-trained DeepGaze model (Python side) # This step is highly dependent on the specific DeepGaze API. # It might involve specifying model paths, device (CPU/GPU), etc. deepgaze_model_instance <- NULL # Global variable to hold the model instance  # Helper function to initialize the model (Python side) ONCE initialize_deepgaze_model <- function(model_weights_path = \"path/to/deepgaze_weights.pth\",                                       device = \"cpu\") { # \"cuda\" for GPU   if (!is.null(deepgaze_model_instance) && inherits(deepgaze_model_instance, \"python.builtin.object\")) {     message(\"DeepGaze model already initialized.\")     return(deepgaze_model_instance)   }    message(\"Initializing DeepGaze model instance...\")   # This is HYPOTHETICAL - adapt to your DeepGaze model's API   tryCatch({     # Example: Assuming the imported module has a function like `load_model`     # or a class constructor.     # deepgaze_model_instance <<- deepgaze_module$load_pretrained_model(weights_path = model_weights_path, device = device)     # OR for a class:     # deepgaze_model_instance <<- deepgaze_module$DeepGazeIII(pretrained=TRUE, device=device) # Hypothetical          # --- REPLACE THIS BLOCK WITH ACTUAL DEEPGAZE MODEL LOADING CODE ---     # For demonstration, let's assume a placeholder function exists in the Python module     # You would replace 'PlaceholderDeepGazeModel' and its arguments.     # Ensure your Python 'deepgaze.pytorch_model_for_example' has this.     if(\"PlaceholderDeepGazeModel\" %in% names(deepgaze_module)) {         deepgaze_model_instance <<- deepgaze_module$PlaceholderDeepGazeModel(device = device)         deepgaze_model_instance$eval() # Typically set model to evaluation mode         message(\"DeepGaze model instance initialized and set to eval mode.\")     } else {         warning(\"PlaceholderDeepGazeModel not found in the imported module. Model not loaded.\")         message(\"You MUST replace the model loading logic with your actual DeepGaze API.\")     }     # --- END OF REPLACEABLE BLOCK ---    }, error = function(e) {     message(\"Error initializing DeepGaze model instance in Python:\")     stop(e)   })   return(deepgaze_model_instance) }   # 3. R function to get saliency map for an image #' Get Saliency Map using DeepGaze #' #' Loads an image, preprocesses it, gets saliency prediction from DeepGaze, #' and returns the saliency map as an R matrix or imager::cimg object. #' #' @param image_path Character string, path to the input image file. #' @param model_instance A Python object representing the loaded DeepGaze model. #' @param target_size Numeric vector c(width, height), e.g., c(1024, 768), #'                    to resize the saliency map to match FDMs. #'                    If NULL, original saliency map size is returned. #' @return A matrix or imager::cimg object representing the saliency map, #'         normalized to sum to 1. Returns NULL on error. #' get_saliency_deepgaze <- function(image_path, model_instance, target_size = NULL) {   if (is.null(model_instance) || !inherits(model_instance, \"python.builtin.object\")) {     message(\"DeepGaze model instance is not valid. Initialize it first.\")     return(NULL)   }   if (!file.exists(image_path)) {     message(\"Image file not found: \", image_path)     return(NULL)   }    PIL <- reticulate::import(\"PIL.Image\", delay_load = TRUE)   transforms <- reticulate::import(\"torchvision.transforms\", delay_load = TRUE) # If needed for preprocessing   torch <- reticulate::import(\"torch\", delay_load = TRUE) # If manual tensor ops needed    tryCatch({     # Load image using PIL (Python Imaging Library)     img_pil <- PIL$open(image_path)$convert('RGB')      # --- REPLACE THIS BLOCK WITH ACTUAL DEEPGAZE PREPROCESSING AND PREDICTION ---     # Preprocessing: This is highly model-specific.     # Example: typical PyTorch preprocessing     # preprocess <- transforms$Compose(list(     #   transforms$Resize(list(224L, 224L)), # Example size     #   transforms$ToTensor(),     #   transforms$Normalize(mean=c(0.485, 0.456, 0.406), std=c(0.229, 0.224, 0.225))     # ))     # img_tensor <- preprocess(img_pil)$unsqueeze(0L) # Add batch dimension      # If your model takes a raw PIL image or numpy array, adapt accordingly.     # For our placeholder, let's assume it takes a PIL image and returns a tensor.     # This part is CRITICALLY dependent on your specific DeepGaze model's API.          # Get prediction (output is usually a PyTorch tensor)     # saliency_tensor_pred <- model_instance$predict(img_tensor) # Hypothetical predict method          # Using our placeholder method that might exist in the Python module     # Ensure your Python 'deepgaze.pytorch_model_for_example' has this.     if(\"predict_saliency_from_pil\" %in% names(model_instance)) {         saliency_tensor_pred <- model_instance$predict_saliency_from_pil(img_pil)     } else {         warning(\"predict_saliency_from_pil method not found on model instance. Cannot predict.\")         message(\"You MUST replace prediction logic with your actual DeepGaze API.\")         return(NULL)     }     # --- END OF REPLACEABLE BLOCK ---       # Postprocess: Convert tensor to R matrix, resize, normalize     # Squeeze batch and channel dimensions if necessary (common for saliency maps)     saliency_map_py <- saliency_tensor_pred$squeeze()$cpu()$detach()$numpy() # To NumPy array      # Convert to R matrix     saliency_map_r <- as.matrix(saliency_map_py)      # Resize if target_size is provided (using imager for convenience)     if (!is.null(target_size) && requireNamespace(\"imager\", quietly = TRUE)) {       saliency_cimg <- imager::as.cimg(saliency_map_r)       # imager's resize uses x,y,z,c dimensions. Saliency is usually 2D.       # Ensure aspect ratio is handled as desired (e.g., stretch vs. pad)       saliency_cimg_resized <- imager::resize(saliency_cimg,                                               size_x = target_size[1],                                               size_y = target_size[2],                                               interpolation_type = 6) # Lanczos for quality       saliency_map_r <- as.matrix(saliency_cimg_resized)     }      # Normalize to sum to 1     saliency_map_r <- saliency_map_r - min(saliency_map_r) # Ensure non-negative     if (sum(saliency_map_r) > 1e-6) {       saliency_map_r <- saliency_map_r / sum(saliency_map_r)     } else {       # Handle case of all-zero map (e.g., return uniform distribution)       saliency_map_r[,] <- 1 / (nrow(saliency_map_r) * ncol(saliency_map_r))       warning(\"Saliency map was near zero; returning uniform distribution.\")     }      return(saliency_map_r)    }, error = function(e) {     message(paste(\"Error getting saliency map for\", image_path, \":\"))     message(e)     return(NULL)   }) }  # --- Example Usage (Illustrative - requires actual DeepGaze setup) --- # # # A. One-time setup (or at start of script/project): # # 1. Ensure you have a Conda env named \"r_deepgaze_env\" with Python, PyTorch, # #    Pillow, NumPy, and your specific DeepGaze package installed. # #    You might use install_deepgaze_env() CAREFULLY or set it up manually. # # # # install_deepgaze_env(deepgaze_package_name = \"actual_pip_name_or_git_url_for_deepgaze\") # # # 2. Activate the environment (already done at the top of this script section) # # reticulate::use_condaenv(\"r_deepgaze_env\", required = TRUE) # # # 3. Initialize the DeepGaze model (Python side) # #    Replace \"path/to/deepgaze_weights.pth\" with the actual path if your model # #    requires loading weights explicitly. Some models load them automatically if pretrained. # #    The Python module `deepgaze.pytorch_model_for_example` and the class/function # #    `PlaceholderDeepGazeModel` and its method `predict_saliency_from_pil` # #    ARE HYPOTHETICAL and need to be replaced with your actual DeepGaze model's API. # # # dg_model <- initialize_deepgaze_model(device = \"cpu\") # or \"cuda\" # # # B. For each image: # # if (!is.null(dg_model)) { # #   # Define target dimensions for the saliency map, e.g., matching your screen/FDM resolution # #   screen_width <- 1024 # #   screen_height <- 768 # #   saliency_map <- get_saliency_deepgaze( # #     image_path = \"path/to/your/stimulus_image.jpg\", # #     model_instance = dg_model, # #     target_size = c(screen_width, screen_height) # #   ) # # # #   if (!is.null(saliency_map)) { # #     # Now 'saliency_map' is an R matrix, normalized, and ready for use # #     # in the Signed EMD calculation (see Addendum A) # #     print(paste(\"Saliency map dimensions:\", paste(dim(saliency_map), collapse=\"x\"))) # #     print(paste(\"Sum of saliency map:\", sum(saliency_map))) # Should be 1 # # # #     if (requireNamespace(\"imager\", quietly = TRUE) && interactive()) { # #        # Quick plot if imager is available # #        # plot(imager::as.cimg(saliency_map), main=\"DeepGaze Saliency Map\") # #     } # #   } # # } else { # #   message(\"DeepGaze model not initialized. Cannot generate saliency map.\") # # }"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Multimatch.html","id":"multimatch","dir":"Articles","previous_headings":"","what":"Multimatch","title":"Multimatch","text":"MultiMatch (Dewhurst et al. 2012) comprehensive method analyzing comparing eye movement scanpaths. evaluates scanpath similarity across five dimensions: “vector”, “direction”, “length”, “position”, “duration”. breaking eye movements distinct components, MultiMatch provides detailed nuanced comparison visual behavior. method particularly useful studies psychology, cognitive science, human-computer interaction understanding visual attention patterns crucial. implementation, add another metric based earth mover’s distance (EMD; ‘position_emd’) positions fixations. capture similarities distributions fixations order-specific. Let’s simulate simple eye-movement pattern compare one another.  Now, compare two fixation groups using multimatch function. metrics give pretty high values, consistent known similarity scanpaths. simulate 8000 pairs randomly generated scanpaths show distribution similarity scores metric.  Let’s plot fixations extreme values multimatch metrics. examining highest lowest pairs can gain insight metric capturing. Note size points captures duration fixation. highest pairs scanpaths metric.  lowest.","code":"# Simulate fixation data simulate_fixations_linear <- function(n) {   out <- data.frame(     fixation = 1:n,     x = cumsum(rnorm(n, mean = 50, sd = 10)),     y = cumsum(rnorm(n, mean = 50, sd = 10)),     duration = rpois(n, lambda = 200)   )      fixation_group(out$x, out$y, out$duration, out$fixation) }  fixations1 <- simulate_fixations_linear(10) fixations2 <- simulate_fixations_linear(12)  p1 <- plot(fixations1) p2 <- plot(fixations2) p1 + p2 sp1 <- scanpath(fixations1) sp2 <- scanpath(fixations2) eyesim:::multi_match(sp1, sp2, c(500,500)) ##       mm_vector    mm_direction       mm_length     mm_position     mm_duration  ##       0.9930345       0.9755456       0.9916165       0.7145947       0.9462729  ## mm_position_emd  ##       0.9167082 # Simulate fixation data for each scenario simulate_fixations_zigzag <- function() {   out <- data.frame(     fixation = 1:10,     x = cumsum(c(50, 50, 50, 50, 50, 50, 50, 50, 50, 50)),     y = cumsum(c(50, -50, 50, -50, 50, -50, 50, -50, 50, -50)),     duration = rpois(10, lambda = 200)   )   fixation_group(out$x, out$y, out$duration, out$fixation) }   fixations1_linear <- simulate_fixations_linear(10) fixations2_zigzag <- simulate_fixations_zigzag() sp1 <- scanpath(fixations1_linear) sp2 <- scanpath(fixations2_zigzag)  eyesim:::multi_match(sp1, sp2, c(500,500)) ##       mm_vector    mm_direction       mm_length     mm_position     mm_duration  ##       0.9400523       0.5882368       0.9884380       0.6518825       0.9247630  ## mm_position_emd  ##       0.6397256 fgs <- replicate(8000, {   fixations1 <- fixation_group(runif(10)*500, runif(10)*500, round(runif(10)*10)+1, 1:10)   fixations2 <- fixation_group(runif(10)*500, runif(10)*500, round(runif(10)*10)+1, 1:10)   list(fg1=fixations1, fg2=fixations2)   #sp1 <- scanpath(fixations1)   #sp2 <- scanpath(fixations2)   #eyesim:::multi_match(sp1, sp2, c(500,500)) }, simplify=FALSE)  ssdf <- lapply(fgs, function(x) eyesim:::multi_match(scanpath(x$fg1), scanpath(x$fg2), c(500,500))) %>% bind_rows() # Create ridgeline plots for each MultiMatch metric ridgeline_vector <- ggplot(ssdf, aes(x = mm_vector, y = factor(1, labels = \"Vector Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'blue', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  ridgeline_direction <- ggplot(ssdf, aes(x = mm_direction, y = factor(1, labels = \"Direction Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'green', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  ridgeline_length <- ggplot(ssdf, aes(x = mm_length, y = factor(1, labels = \"Length Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'red', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  ridgeline_position <- ggplot(ssdf, aes(x = mm_position, y = factor(1, labels = \"Position Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'purple', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  ridgeline_duration <- ggplot(ssdf, aes(x = mm_duration, y = factor(1, labels = \"Duration Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'orange', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  ridgeline_emd <- ggplot(ssdf, aes(x = mm_position_emd, y = factor(1, labels = \"EMD Similarity\"))) +    geom_density_ridges(scale = 3, rel_min_height = 0.01, bandwidth = 0.05, fill = 'cyan', color = 'black', alpha = 0.7) +    labs(x = 'Similarity', y = '') + theme_ridges() + theme(axis.title.y = element_blank())  # Combine the ridgeline plots using patchwork combined_plot <- ridgeline_vector / ridgeline_direction / ridgeline_length / ridgeline_position / ridgeline_duration / ridgeline_emd +    plot_layout(ncol = 1)  # Display the combined plot print(combined_plot) metrics <- c(\"vector\", \"direction\", \"length\", \"position\", \"duration\", \"position_emd\")  extremes <- lapply(metrics, function(metric) {   m <- paste0(\"mm_\", metric)   list(     high = which.max(ssdf[[m]]),     low = which.min(ssdf[[m]])   ) })  names(extremes) <- metrics  # Function to create a plot for a fixation group with a title and annotation plot_fixation_group <- function(fg, title, metric_value) {   plot(fg) +      ggtitle(title) +      annotate(\"text\", x = Inf, y = -Inf, label = paste(\"score:\", round(metric_value, 2)),               hjust = 1.1, vjust = -1.1, size = 2, color = \"blue\") }  # Create plots for each metric plots <- lapply(names(extremes), function(metric) {   high_index <- extremes[[metric]]$high   low_index <- extremes[[metric]]$low      metric_high_value <- ssdf[high_index, paste0(\"mm_\", metric)]   metric_low_value <- ssdf[low_index, paste0(\"mm_\", metric)]    high_fg1 <- fgs[[high_index]]$fg1   high_fg2 <- fgs[[high_index]]$fg2   low_fg1 <- fgs[[low_index]]$fg1   low_fg2 <- fgs[[low_index]]$fg2    high_plot1 <- plot_fixation_group(high_fg1, paste(metric, \"High(1)\"), metric_high_value)   high_plot2 <- plot_fixation_group(high_fg2, paste(metric, \"High(2)\"), metric_high_value)   low_plot1 <- plot_fixation_group(low_fg1, paste(metric, \"Low(1)\"), metric_low_value)   low_plot2 <- plot_fixation_group(low_fg2, paste(metric, \"Low(2)\"), metric_low_value)    list(high_plot = high_plot1 + high_plot2, low_plot = low_plot1 + low_plot2) })  # Set the size of each plot plot_height <- 12 # Adjust height as needed plot_width <- 12 # Adjust width as needed  # Combine and arrange plots using patchwork high_plots <- wrap_plots(   plots[[1]]$high_plot,   plots[[2]]$high_plot,   plots[[3]]$high_plot,   plots[[4]]$high_plot,   plots[[5]]$high_plot,   plots[[6]]$high_plot,   nrow = 6 ) + plot_layout(guides = 'collect', heights = rep(plot_height, 6)) + ggtitle(\"High MultiMatch Scores\")  low_plots <- wrap_plots(   plots[[1]]$low_plot,   plots[[2]]$low_plot,   plots[[3]]$low_plot,   plots[[4]]$low_plot,   plots[[5]]$low_plot,   plots[[6]]$low_plot,   nrow = 6 ) + plot_layout(guides = 'collect', heights = rep(plot_height, 6)) + ggtitle(\"Low MultiMatch Scores\")  # Save the plots to file #ggsave(\"high_plots.png\", plot = high_plots, height = plot_height * 5, width = plot_width, units = \"in\") #ggsave(\"low_plots.png\", plot = low_plots, height = plot_height * 5, width = plot_width, units = \"in\") high_plots low_plots"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Multimatch.html","id":"how-transofrmations-impact-multimatch-metrics","dir":"Articles","previous_headings":"","what":"How transofrmations impact Multimatch metrics","title":"Multimatch","text":"Let’s generate random scanpath apply transformations see multimatch metrics change.  Now, scale second scanpath .5 recompute multimatch metrics. notice “direction” metric remains 1, position (especially) decreases dramatically. absolute positions fixations preserved scaling.  Next preserve positions fixations scramble order. large impact “direction” metric, metrics. Note also earth mover’s distance remain high.","code":"fg <- fixation_group(runif(10)*500, runif(10)*500, round(runif(10)*10)+1, 1:10)  plot(fg) + plot(fg) eyesim:::multi_match(scanpath(fg), scanpath(fg), c(500,500)) ##       mm_vector    mm_direction       mm_length     mm_position     mm_duration  ##               1               1               1               1               1  ## mm_position_emd  ##               1 fg2 <- fg fg2$y <- fg2$y * .5 fg2$x <- fg2$x * .5  p1 <- plot(fg) + ylim(0,500) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p2 <- plot(fg2) + ylim(0,500) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p1+p2 eyesim:::multi_match(scanpath(fg), scanpath(fg2), c(500,500)) ##       mm_vector    mm_direction       mm_length     mm_position     mm_duration  ##       0.9300530       1.0000000       0.8601060       0.8158216       1.0000000  ## mm_position_emd  ##       0.8000760 fg2 <- fg ord <- sample(1:nrow(fg2)) fg2$x <- fg2$x[ord] fg2$y <- fg2$y[ord] fg2$duration <- fg2$duration[ord] p1 <- plot(fg) + ylim(0,500) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p2 <- plot(fg2) + ylim(0,500) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p1+p2 eyesim:::multi_match(scanpath(fg), scanpath(fg2), c(500,500)) ##       mm_vector    mm_direction       mm_length     mm_position     mm_duration  ##       0.8242253       0.5280783       0.9153703       0.6467169       0.3750000  ## mm_position_emd  ##       0.9945680"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Multimatch.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Multimatch","text":"Dewhurst, R., Nyström, M., Jarodzka, H., Foulsham, T., Johansson, R., & Holmqvist, K. (2012). depends look : Scanpath comparison multiple dimensions MultiMatch, vector-based approach. Behavior Research Methods, 44, 1079-1100.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Overview.html","id":"eye-movement-similarity-analysis","dir":"Articles","previous_headings":"","what":"Eye-movement similarity analysis","title":"Overview","text":"main goal eyesim provide tools computing measures similarity eye-movement fixation data collected series trials embedded experimental design. major focus library offer easy ways compare fixation patterns two experimental states, example, perceiving image remembering image. kinds analyses useful assessing quantifying -called “eye-movement reinstatement” studies memory. describe basic aspects library allow one get started .","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Overview.html","id":"a-basic-unit-the-fixation-group","dir":"Articles","previous_headings":"","what":"A Basic unit: the fixation group","title":"Overview","text":"fixation group set eye-movement fixations comprise meaningful unit study, example, trial, condition, participants, time window, etc. Every fixation group contains set xy coordinates corresponding vectors indicating onset (fixation start?) duration (long fixation last?) sequence fixations. create fixation group set 3 coordinates occurring times 0, 10, 60. creating fixation_group object, plot visualize location three fixations. size point scaled duration color point mapped onset time, yellow colors early-ocurring red colors late-occurring group.  Now create larger group consisting 25 randomly generated eye-movements.  ’s pretty busy display, captures sequence eye-movements. can also plot various kinds “density” maps show fixations likely occur. show three different ways plotting fixation density: contour plot, raster plot, filled contour plot.  can also vary bandwidth two-dimensional density estimation procedure visualize fixations different levels smoothness.","code":"library(eyesim) library(patchwork) library(dplyr) fg <- fixation_group(x=c(-100, 0, 100), y=c(0, 100, 0), onset=c(0,10,60), duration=c(10,50,100)) plot(fg) ## generate random fixation coordinates cds <- do.call(rbind, lapply(1:25, function(i) {   data.frame(x=runif(1)*100, y=runif(1)*100) }))  ## generate random increasing onsets onset <- cumsum(runif(25)*100)  ## construct a \"fixation_group\" object fg <- fixation_group(x=cds[,1], y=cds[,2], onset=onset, duration=c(diff(onset),25)) plot(fg) p1 <- plot(fg, typ=\"contour\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=35) p2 <- plot(fg, typ=\"raster\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=35) p3 <- plot(fg, typ=\"filled_contour\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=35)  p1+p2+p3 #> Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0. #> ℹ Please use `after_stat(level)` instead. #> ℹ The deprecated feature was likely used in the eyesim package. #>   Please report the issue to the authors. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. #> Warning: Removed 396 rows containing missing values or values outside the scale range #> (`geom_raster()`). p1 <- plot(fg, typ=\"filled_contour\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=20) p2 <- plot(fg, typ=\"filled_contour\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=60) p3 <- plot(fg, typ=\"filled_contour\", xlim=c(-10,110), ylim=c(-10,110), bandwidth=100)  p1+p2+p3"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Overview.html","id":"computing-similarity-between-fixation-groups","dir":"Articles","previous_headings":"","what":"Computing similarity between fixation groups","title":"Overview","text":"Suppose two fixation groups, fg1 fg2, compare coordinate sets? eyesim package provides methods computing similarities spatial density maps. , generate two eye-movement patterns, one perturbed version . compute series similarity metrics two patterns.  compute similarity two fixation_groups use similarity generic function. First convert fixation_groups eye_density objects compute similarity. default metric comparing two density maps Pearson correlation coefficient. can compute similarity measures well. compute similarity using Pearson correlation, Spearman correlation, Fisher-transformed Pearson correlation, cosine similarity, “l1” similarity based 1-norm, Jaccard similarity (proxy package), distance covariance (dcov energy package).","code":"cds <- do.call(rbind, lapply(1:25, function(i) {   data.frame(x=runif(1)*100, y=runif(1)*100) }))  cds2 <- do.call(rbind, lapply(1:25, function(i) {   if (i %% 2 == 0) {     data.frame(x=runif(1)*100, y=runif(1)*100)   } else {     data.frame(x=cds[i,1], y=cds[i,2])   } }))  onset <- cumsum(runif(25)*100) fg1 <- fixation_group(x=cds[,1], y=cds[,2], onset=onset, duration=c(diff(onset),25)) fg2 <- fixation_group(x=cds2[,1], y=cds2[,2], onset=onset, duration=c(diff(onset),25))  p1 <- plot(fg1) p2 <- plot(fg2) p1+p2 ed1 <- eye_density(fg1, sigma=50, xbounds=c(0,100), ybounds=c(0,100)) ed2 <- eye_density(fg2, sigma=50, xbounds=c(0,100), ybounds=c(0,100)) ed3 <- eye_density(fg2, sigma=50, xbounds=c(0,100), ybounds=c(0,100), duration_weighted = TRUE)  s1 <- similarity(ed1,ed2) s1 #> [1] 0.808746 methods=c(\"pearson\", \"spearman\", \"fisherz\", \"cosine\", \"l1\", \"jaccard\", \"dcov\") for (meth in methods) {   s1 <- similarity(ed1,ed2, method=meth)   print(paste(meth, \":\", s1)) } #> [1] \"pearson : 0.808746026103112\" #> [1] \"spearman : 0.809895263806257\" #> [1] \"fisherz : 1.12339341493312\" #> [1] \"cosine : 0.994979279138522\" #> [1] \"l1 : 0.954777793980802\" #> [1] \"jaccard : 0.990007122635681\" #> [1] \"dcov : 0.774013464858466\""},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Overview.html","id":"computing-similarity-between-set-of-fixation-groups-in-an-experiment","dir":"Articles","previous_headings":"","what":"Computing similarity between set of fixation groups in an experiment","title":"Overview","text":"Suppose memory experiment images presented “encoding” block also retrieval/recognition block. images repeated (cued way) subjects asked recognize (recall) images. situation, might want compute pairwise similarity encoding retrieval trials image repeated. might also want control non-specific eye-movement similarity two arbitrary encoding retrieval trials. simulate data experiment 50 images shown encoding block 50 images shown (cued) retrieval block. compute eye-movement similarity encoding retrieval trials corresponding imege average similarity set non-corresponding encoding retrieval trials. generate data three participants, 50 encoding 50 retrieval trials. use eye_table data structure, extension data.frame hold data. code , function gen_fixations generates number fixations (1 10) randomly distributed 100 100 coordinate frame. Although fixations every trial randomly selected, assign experimental condition (“encoding”, “retrieval”) subject id (“s1”, “s2”, “s3”) set generated coordinates. Now ready create eye_table data structure stores fixations associates experimental design grouping structure. group fixations “image”, “participant”, “phase”. allow sets fixations grouped together fixation_groups eye-movement similarity analyses can carried . Without grouping variables, way know pool eye-movements together sets appropriate kernel density estimation analyses. Next, compute similarity encoding-retrieval pairs, pair consists image viewed encoding retrieval, respectively. essence, need “match” retrieval trial corresponding encoding trial, viewed images conditions . first step compute “density maps” fixation_group, defined intersection participant, image, phase variables stored eyetab object. compute density maps combinations participant, image, phase plot first four density maps resulting set.  print first 10 rows resulting data.frame, contains eye_density objects stored density variable. Now set eye_density maps, can run similarity analysis. , use template_similarity function. want compare “retrieval” density map corresponding encoding density map (“template”). want “match” name image first studied (“encoding”) later recognized (“retrieval”). First, split encoding retrieval trials using dplyr::filter method. Next call template_similarity indicating pairs matched image variable. choose fisherz similarity measure correct non-specific eye-movement similarity using 50 permutations similarity computed non-matching image pairs subtracted similarity score. raw similarity score returned eye_sim permutation-corrected score returned eye_sim_diff. similarity score among permuted pairs also returned perm_sim. eye-movement fixations generated randomly, expect non-zero similarity score eye_sim_diff variable, can test one-sample t-test. expected, t-test significant. can also plot histograms similarity scores.","code":"gen_fixations <- function(imname, phase, trial, participant) {   ## generate some number of fixation between 1 and 10   nfix <- ceiling(runif(1) * 10)   cds <- do.call(rbind, lapply(1:nfix, function(i) {     data.frame(x=runif(1)*100, y=runif(1)*100)   }))      onset <- cumsum(runif(nfix)*100)   df1 <- data.frame(x=cds[,1], y=cds[,2], onset=onset,                      duration=c(diff(onset),100), image=imname,                      phase=phase, trial=trial, participant=participant)  }  df1 <- lapply(c(\"s1\", \"s2\", \"s3\"), function(snum) {   lapply(c(\"encoding\", \"retrieval\"), function(phase) {     lapply(paste0(1:50), function(trial) {       gen_fixations(paste0(\"im\", trial), phase, trial, snum)     }) %>% bind_rows()   }) %>% bind_rows() }) %>% bind_rows() eyetab <- eye_table(\"x\", \"y\", \"duration\", \"onset\", groupvar=c(\"participant\", \"phase\", \"image\"), data=df1) #> # A tibble: 300 × 4 #> # Groups:   participant, phase, image [300] #>    participant phase    image fixgroup           #>    <chr>       <chr>    <chr> <list>             #>  1 s1          encoding im1   <fxtn_grp [5 × 6]> #>  2 s1          encoding im10  <fxtn_grp [4 × 6]> #>  3 s1          encoding im11  <fxtn_grp [4 × 6]> #>  4 s1          encoding im12  <fxtn_grp [2 × 6]> #>  5 s1          encoding im13  <fxtn_grp [9 × 6]> #>  6 s1          encoding im14  <fxtn_grp [9 × 6]> #>  7 s1          encoding im15  <fxtn_grp [5 × 6]> #>  8 s1          encoding im16  <fxtn_grp [6 × 6]> #>  9 s1          encoding im17  <fxtn_grp [6 × 6]> #> 10 s1          encoding im18  <fxtn_grp [4 × 6]> #> # ℹ 290 more rows eyedens <- density_by(eyetab, groups=c(\"phase\", \"image\", \"participant\"), sigma=100, xbounds=c(0,100), ybounds=c(0,100))  p1 <- plot(eyedens$density[[1]]) p2 <- plot(eyedens$density[[2]]) p3 <- plot(eyedens$density[[3]]) p4 <- plot(eyedens$density[[4]])  (p1 + p2) / (p3+ p4) eyedens #> # A tibble: 276 × 5 #>    phase    image participant fixgroup            density        #>    <chr>    <chr> <chr>       <list>              <list>         #>  1 encoding im1   s1          <fxtn_grp [5 × 6]>  <ey_dnsty [5]> #>  2 encoding im1   s2          <fxtn_grp [10 × 6]> <ey_dnsty [5]> #>  3 encoding im1   s3          <fxtn_grp [2 × 6]>  <ey_dnsty [5]> #>  4 encoding im10  s1          <fxtn_grp [4 × 6]>  <ey_dnsty [5]> #>  5 encoding im10  s2          <fxtn_grp [7 × 6]>  <ey_dnsty [5]> #>  6 encoding im11  s1          <fxtn_grp [4 × 6]>  <ey_dnsty [5]> #>  7 encoding im11  s2          <fxtn_grp [3 × 6]>  <ey_dnsty [5]> #>  8 encoding im11  s3          <fxtn_grp [8 × 6]>  <ey_dnsty [5]> #>  9 encoding im12  s1          <fxtn_grp [2 × 6]>  <ey_dnsty [5]> #> 10 encoding im12  s2          <fxtn_grp [3 × 6]>  <ey_dnsty [5]> #> # ℹ 266 more rows set.seed(1234) enc_dens <- eyedens %>% filter(phase == \"encoding\") ret_dens <- eyedens %>% filter(phase == \"retrieval\")  simres1 <- template_similarity(enc_dens, ret_dens, match_on=\"image\", method=\"fisherz\", permutations=50) #> template_similarity: similarity metric is fisherz  t.test(simres1$eye_sim_diff) #>  #>  One Sample t-test #>  #> data:  simres1$eye_sim_diff #> t = 1.0709, df = 138, p-value = 0.2861 #> alternative hypothesis: true mean is not equal to 0 #> 95 percent confidence interval: #>  -0.0510963  0.1718359 #> sample estimates: #>  mean of x  #> 0.06036983 par(mfrow=c(1,3)) hist(simres1$eye_sim, main=\"raw eye movement similarity\") hist(simres1$perm_sim, main=\"image-permuted eye movement similarity\") hist(simres1$eye_sim_diff, main=\"corrected eye movement similarity\")"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/Overview.html","id":"multiscale-analysis","dir":"Articles","previous_headings":"","what":"Multiscale analysis","title":"Overview","text":"Rather choosing single bandwidth (sigma) value, can perform multiscale analysis providing vector sigma values. approach avoids need arbitrarily select single smoothing parameter instead captures similarity across multiple spatial scales. multiscale approach provides robust similarity estimate averaging across different levels spatial smoothing.","code":"# Compute multiscale density maps using multiple sigma values eyedens_multi <- density_by(eyetab, groups=c(\"phase\", \"image\", \"participant\"),                             sigma=c(25, 50, 100), xbounds=c(0,100), ybounds=c(0,100))  enc_dens_multi <- eyedens_multi %>% filter(phase == \"encoding\") ret_dens_multi <- eyedens_multi %>% filter(phase == \"retrieval\")  # Run template similarity analysis with multiscale data simres_multi <- template_similarity(enc_dens_multi, ret_dens_multi,                                    match_on=\"image\", method=\"fisherz\", permutations=50) #> template_similarity: similarity metric is fisherz  # Compare single-scale vs multiscale results cat(\"Single-scale mean similarity:\", round(mean(simres1$eye_sim_diff), 4), \"\\n\") #> Single-scale mean similarity: 0.0604 cat(\"Multiscale mean similarity:\", round(mean(simres_multi$eye_sim_diff), 4), \"\\n\") #> Multiscale mean similarity: 0.0644"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/RepetitiveSimilarity.html","id":"repetitive-similarity","dir":"Articles","previous_headings":"","what":"Repetitive Similarity","title":"Repetitive Similarity","text":"repetitive_similarity function computes eye-movement similarity datasets stimulus viewed multiple times across experimental conditions. particularly useful memory experiments images presented encoding later retrieval phases. Rather comparing specific encoding-retrieval pairs, repetitive similarity examines possible within-stimulus combinations across conditions computes summary statistics (mean, median, etc.) characterize overall similarity pattern.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/articles/RepetitiveSimilarity.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Repetitive Similarity","text":"Let’s simulate simple dataset images viewed encoding retrieval: Now compute repetitive similarity:","code":"# Generate sample fixation data gen_fixations <- function(imname, phase, trial, participant) {   nfix <- ceiling(runif(1) * 8) + 2  # 2-10 fixations   cds <- data.frame(x = runif(nfix) * 100, y = runif(nfix) * 100)   onset <- cumsum(runif(nfix) * 50)      data.frame(     x = cds$x, y = cds$y, onset = onset,     duration = c(diff(onset), 50),     image = imname, phase = phase,      trial = trial, participant = participant   ) }  # Create dataset: 2 participants, 3 images, encoding + retrieval df <- lapply(c(\"s1\", \"s2\"), function(snum) {   lapply(c(\"encoding\", \"retrieval\"), function(phase) {     lapply(paste0(\"img\", 1:3), function(img) {       gen_fixations(img, phase, img, snum)     }) %>% bind_rows()   }) %>% bind_rows() }) %>% bind_rows()  # Create eye_table eyetab <- eye_table(\"x\", \"y\", \"duration\", \"onset\",                     groupvar = c(\"participant\", \"phase\", \"image\"),                     data = df)  # Compute density maps eyedens <- density_by(eyetab, groups = c(\"phase\", \"image\", \"participant\"),                       sigma = 50, xbounds = c(0, 100), ybounds = c(0, 100)) # Run repetitive similarity analysis rep_sim <- repetitive_similarity(eyedens,                                 condition_var = \"phase\",                                method = \"pearson\")  print(rep_sim) #> # A tibble: 12 × 7 #>    phase     image participant fixgroup           density        repsim othersim #>    <chr>     <chr> <chr>       <list>             <list>          <dbl>    <dbl> #>  1 encoding  img1  s1          <fxtn_grp [3 × 6]> <ey_dnsty [5]>  0.368    0.484 #>  2 encoding  img1  s2          <fxtn_grp [6 × 6]> <ey_dnsty [5]>  0.591    0.413 #>  3 encoding  img2  s1          <fxtn_grp [9 × 6]> <ey_dnsty [5]>  0.649    0.636 #>  4 encoding  img2  s2          <fxtn_grp [7 × 6]> <ey_dnsty [5]>  0.683    0.510 #>  5 encoding  img3  s1          <fxtn_grp [4 × 6]> <ey_dnsty [5]>  0.660    0.540 #>  6 encoding  img3  s2          <fxtn_grp [3 × 6]> <ey_dnsty [5]>  0.748    0.643 #>  7 retrieval img1  s1          <fxtn_grp [5 × 6]> <ey_dnsty [5]>  0.533    0.626 #>  8 retrieval img1  s2          <fxtn_grp [8 × 6]> <ey_dnsty [5]>  0.220    0.514 #>  9 retrieval img2  s1          <fxtn_grp [7 × 6]> <ey_dnsty [5]>  0.557    0.685 #> 10 retrieval img2  s2          <fxtn_grp [9 × 6]> <ey_dnsty [5]>  0.388    0.625 #> 11 retrieval img3  s1          <fxtn_grp [3 × 6]> <ey_dnsty [5]>  0.189    0.482 #> 12 retrieval img3  s2          <fxtn_grp [3 × 6]> <ey_dnsty [5]>  0.238    0.294"},{"path":"https://bbuchsbaum.github.io/eyesim/articles/RepetitiveSimilarity.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualization","title":"Repetitive Similarity","text":"repetitive_similarity function provides straightforward way quantify consistently participants look locations viewing repeated stimuli across different experimental phases.","code":"# Plot the similarity results ggplot(rep_sim, aes(x = image, y = repsim)) +   geom_col(fill = \"steelblue\", alpha = 0.7) +   labs(x = \"Stimulus\", y = \"Repetitive Similarity\",        title = \"Eye-movement Similarity Within Phase\") +   theme_minimal() # Also plot other similarity ggplot(rep_sim, aes(x = image, y = othersim)) +   geom_col(fill = \"coral\", alpha = 0.7) +   labs(x = \"Stimulus\", y = \"Other Similarity\",         title = \"Eye-movement Similarity Across Different Phases\") +   theme_minimal()"},{"path":"https://bbuchsbaum.github.io/eyesim/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bradley Buchsbaum. Maintainer.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Buchsbaum B (2025). eyesim: analysis eye-movement data. R package version 0.1.0, https://bbuchsbaum.github.io/eyesim/.","code":"@Manual{,   title = {eyesim: analysis of eye-movement data},   author = {Bradley Buchsbaum},   year = {2025},   note = {R package version 0.1.0},   url = {https://bbuchsbaum.github.io/eyesim/}, }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"analysis of eye-movement data","text":"can install development version GitHub :","code":"#install.packages(\"remotes\") remotes::install_github(\"bbuchsbaum/eyesim\")"},{"path":"https://bbuchsbaum.github.io/eyesim/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"analysis of eye-movement data","text":"two eye-movement fixation patterns similar? goal eyesim package provide methods answering question. example, studies recognition memory, might ask whether eye-movement fixation pattern first presentation image (“encoding”) similar eye-movement fixation pattern second presentation image (“recognition”). Furthermore, can ask whether fixation similarity varies function conditions experimental design behavioral responses (e.g. correct recognition miss).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"analysis of eye-movement data","text":"See examples use eyesim vignettes. luck might able make cool image like one !","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Scanpath to a Data Frame — add_scanpath.data.frame","title":"Add Scanpath to a Data Frame — add_scanpath.data.frame","text":"function adds scanpath data frame.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Scanpath to a Data Frame — add_scanpath.data.frame","text":"","code":"# S3 method for class 'data.frame' add_scanpath(x, outvar = \"scanpath\", fixvar = \"fixgroup\")"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Scanpath to a Data Frame — add_scanpath.data.frame","text":"x data frame. outvar output variable name scanpath. Defaults \"scanpath\". fixvar fixation group variable name. Defaults \"fixgroup\".","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add Scanpath to a Data Frame — add_scanpath.data.frame","text":"data frame added scanpath.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.data.frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add Scanpath to a Data Frame — add_scanpath.data.frame","text":"","code":"# Create a data frame with a fixation group df <- data.frame(x = 1:5, y = 6:10, fixgroup = rep(1, 5)) # Add a scanpath to the data frame df <- add_scanpath.data.frame(df) #> Error in add_scanpath.data.frame(df): could not find function \"add_scanpath.data.frame\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.eye_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Scanpath to an Eye Table — add_scanpath.eye_table","title":"Add Scanpath to an Eye Table — add_scanpath.eye_table","text":"function adds scanpath eye table.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.eye_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Scanpath to an Eye Table — add_scanpath.eye_table","text":"","code":"# S3 method for class 'eye_table' add_scanpath(x, outvar = \"scanpath\", fixvar = \"fixgroup\")"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.eye_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Scanpath to an Eye Table — add_scanpath.eye_table","text":"x eye table object. outvar output variable name scanpath. Defaults \"scanpath\". fixvar fixation group variable name. Defaults \"fixgroup\".","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.eye_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add Scanpath to an Eye Table — add_scanpath.eye_table","text":"eye table object added scanpath.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.eye_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add Scanpath to an Eye Table — add_scanpath.eye_table","text":"","code":"# Create an eye table with a fixation group df <- data.frame(x = 1:5, y = 6:10, fixgroup = rep(1, 5)) eye_table_df <- as_eye_table(df) # Add a scanpath to the eye table eye_table_df <- add_scanpath.eye_table(eye_table_df) #> Error in add_scanpath.eye_table(eye_table_df): could not find function \"add_scanpath.eye_table\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Scanpath to Dataset — add_scanpath","title":"Add Scanpath to Dataset — add_scanpath","text":"function adds scanpath input dataset.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Scanpath to Dataset — add_scanpath","text":"","code":"add_scanpath(x, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Scanpath to Dataset — add_scanpath","text":"x input dataset scanpath added. ... Additional arguments passed method adds scanpath.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add Scanpath to Dataset — add_scanpath","text":"dataset added scanpath.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/add_scanpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add Scanpath to Dataset — add_scanpath","text":"","code":"# Example usage of the add_scanpath function input_dataset <- # input dataset data # Additional arguments required for the specific method that adds the scanpath updated_dataset <- add_scanpath(input_dataset, ...) #> Error: '...' used in an incorrect context"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/anim_scanpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Animate a Fixation Scanpath with gganimate — anim_scanpath","title":"Animate a Fixation Scanpath with gganimate — anim_scanpath","text":"function creates animated visualization fixation scanpath using gganimate.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/anim_scanpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Animate a Fixation Scanpath with gganimate — anim_scanpath","text":"","code":"anim_scanpath(   x,   bg_image = NULL,   xlim = range(x$x),   ylim = range(x$y),   alpha = 1,   anim_over = c(\"index\", \"onset\"),   type = c(\"points\", \"raster\"),   time_bin = 1 )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/anim_scanpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Animate a Fixation Scanpath with gganimate — anim_scanpath","text":"x `fixation_group` object. bg_image optional image file name use background. xlim range x coordinates (default: range x values fixation group). ylim range y coordinates (default: range y values fixation group). alpha opacity dot (default: 1). anim_over Animate index (ordered) onset (real time) (default: c(\"index\", \"onset\")). type Display points raster (default: c(\"points\", \"raster\")). time_bin size time bins (default: 1).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/anim_scanpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Animate a Fixation Scanpath with gganimate — anim_scanpath","text":"gganimate object representing animated scanpath.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/anim_scanpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Animate a Fixation Scanpath with gganimate — anim_scanpath","text":"","code":"# Create a fixation group fg <- fixation_group(x=c(.1,.5,1), y=c(1,.5,1), onset=1:3, duration=rep(1,3)) # Animate the scanpath for the fixation group anim_sp <- anim_scanpath(fg)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as.data.frame.eye_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","title":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","text":"function converts eye_density object data.frame x, y, z values.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as.data.frame.eye_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","text":"","code":"# S3 method for class 'eye_density' as.data.frame(x, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as.data.frame.eye_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","text":"x eye_density object converted data.frame. ... Additional arguments passed method (currently used).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as.data.frame.eye_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","text":"data.frame columns x, y, z representing x-axis, y-axis, density values, respectively.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as.data.frame.eye_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an eye_density object to a data.frame. — as.data.frame.eye_density","text":"function extracts x y values eye_density object, creates data.frame possible combinations x y using purrr::cross_df(). adds new column 'z' data.frame density values eye_density object.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as_eye_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Reapply the 'eye_table' Class to an Object — as_eye_table","title":"Reapply the 'eye_table' Class to an Object — as_eye_table","text":"`as_eye_table` function simple utility function reapplies 'eye_table' class given object already part class attribute. function intended serious use.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as_eye_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reapply the 'eye_table' Class to an Object — as_eye_table","text":"","code":"as_eye_table(x)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as_eye_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reapply the 'eye_table' Class to an Object — as_eye_table","text":"x input object 'eye_table' class (re)applied.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as_eye_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reapply the 'eye_table' Class to an Object — as_eye_table","text":"input object 'eye_table' class (re)applied class attribute.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/as_eye_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reapply the 'eye_table' Class to an Object — as_eye_table","text":"","code":"# Create a simple data.frame df <- data.frame(x = 1:5, y = 6:10)  # Apply the 'eye_table' class to df eye_table_df <- as_eye_table(df) class(eye_table_df) #> [1] \"eye_table\"  \"data.frame\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/calcangle.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Angle Between Two Vectors — calcangle","title":"Calculate the Angle Between Two Vectors — calcangle","text":"function calculates angle (degrees) two vectors, x1 x2.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/calcangle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Angle Between Two Vectors — calcangle","text":"","code":"calcangle(x1, x2)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/calcangle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Angle Between Two Vectors — calcangle","text":"x1 numeric vector. x2 numeric vector.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/calcangle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Angle Between Two Vectors — calcangle","text":"numeric value representing angle two vectors degrees.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/calcangle.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Angle Between Two Vectors — calcangle","text":"","code":"calcangle(c(1, 2), c(2, 2)) #> [1] 18.43495"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/cart2pol.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","title":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","text":"function converts Cartesian coordinates (x, y) polar coordinates (rho, theta).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/cart2pol.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","text":"","code":"cart2pol(x, y)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/cart2pol.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","text":"x numeric vector representing x-coordinates. y numeric vector representing y-coordinates.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/cart2pol.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","text":"matrix two columns, first column rho (radial coordinate)   second column theta (angular coordinate).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/cart2pol.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Cartesian Coordinates to Polar Coordinates — cart2pol","text":"","code":"cart2pol(c(1, 2), c(2, 2)) #> Error in cart2pol(c(1, 2), c(2, 2)): could not find function \"cart2pol\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/center.html","id":null,"dir":"Reference","previous_headings":"","what":"Center Eye-Movements in a New Coordinate System — center","title":"Center Eye-Movements in a New Coordinate System — center","text":"function centers eye-movements new coordinate system specified origin.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/center.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Center Eye-Movements in a New Coordinate System — center","text":"","code":"center(x, origin, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/center.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Center Eye-Movements in a New Coordinate System — center","text":"x input object containing eye-movements centered. origin vector containing x y coordinates new coordinate system's origin. ... Additional arguments passed centering method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/center.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Center Eye-Movements in a New Coordinate System — center","text":"object containing centered eye-movements new coordinate system.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/center.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Center Eye-Movements in a New Coordinate System — center","text":"","code":"# Example usage of the center function input_object <- # input object data containing eye-movements new_origin <- c(500, 500) # New coordinate system origin centered_object <- center(input_object, origin = new_origin) #> Error in UseMethod(\"center\", x): no applicable method for 'center' applied to an object of class \"c('double', 'numeric')\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/coords.html","id":null,"dir":"Reference","previous_headings":"","what":"extract coordinates — coords","title":"extract coordinates — coords","text":"extract coordinates","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/coords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract coordinates — coords","text":"","code":"coords(x)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/coords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extract coordinates — coords","text":"x object","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_by.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Eye Density by Groups — density_by","title":"Calculate Eye Density by Groups — density_by","text":"function calculates eye density fixations, grouped specified variables.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_by.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Eye Density by Groups — density_by","text":"","code":"density_by(   x,   groups,   sigma = 50,   xbounds = c(0, 1000),   ybounds = c(0, 1000),   outdim = c(100, 100),   duration_weighted = TRUE,   window = NULL,   min_fixations = 2,   keep_vars = NULL,   fixvar = \"fixgroup\",   result_name = \"density\",   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_by.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Eye Density by Groups — density_by","text":"x data frame containing fixations additional grouping variables. groups character vector specifying grouping variables use. sigma numeric value specifying bandwidth kernel density estimation (default 50). xbounds numeric vector length 2 specifying x-axis bounds density calculation (default c(0, 1000)). ybounds numeric vector length 2 specifying y-axis bounds density calculation (default c(0, 1000)). outdim numeric vector length 2 specifying dimensions output density matrix (default c(100, 100)). duration_weighted logical value indicating whether density weighted fixation duration (default TRUE). window numeric vector length 2 specifying time window selecting fixations (default NULL). min_fixations Minimum number fixations required computing density map. Rows fewer fixations receive NULL results. Default 2. keep_vars character vector specifying additional variables keep output (default NULL). fixvar character string specifying name fixation variable (default \"fixgroup\"). result_name character string specifying name density result variable (default \"density\"). ... Additional arguments passed `eye_density` function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_by.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Eye Density by Groups — density_by","text":"data frame containing original grouping variables, fixations, density result.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_by.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Eye Density by Groups — density_by","text":"","code":"# Create a data frame with fixations and a grouping variable fixations <- data.frame(   subject = rep(c(\"A\", \"B\"), each = 25),   x = runif(50, 0, 1000),   y = runif(50, 0, 1000),   duration = rep(1, 50),   onset = seq(1, 50) )  # Calculate eye density by subject result <- density_by(fixations, groups = \"subject\") #> Error in do.call(rbind, df[[fixvar]]): second argument must be a list"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Density Matrix for a Given Object — density_matrix","title":"Compute Density Matrix for a Given Object — density_matrix","text":"function computes density matrix given object, optionally taking account grouping variable.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Density Matrix for a Given Object — density_matrix","text":"","code":"density_matrix(x, groups, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Density Matrix for a Given Object — density_matrix","text":"x input object density matrix computed. groups optional grouping variable consider computing density matrix. ... Additional arguments passed density matrix computation method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Density Matrix for a Given Object — density_matrix","text":"density matrix representing input object, taking account specified grouping variable provided.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/density_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Density Matrix for a Given Object — density_matrix","text":"","code":"# Example usage of the density_matrix function input_object <- # input object data grouping_variable <- # optional grouping variable result_density_matrix <- density_matrix(input_object, groups = grouping_variable) #> Error: object 'input_object' not found"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/estimate_scale.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Scaling Parameters for Fixation Data — estimate_scale","title":"Estimate Scaling Parameters for Fixation Data — estimate_scale","text":"function estimates scaling parameters two sets fixation data using Hausdorff distance optimization objective.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/estimate_scale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Scaling Parameters for Fixation Data — estimate_scale","text":"","code":"estimate_scale(x, y, lower = c(0.1, 0.1), upper = c(10, 10), window)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/estimate_scale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Scaling Parameters for Fixation Data — estimate_scale","text":"x data frame matrix containing x coordinates first set fixations. y data frame matrix containing y coordinates second set fixations. lower numeric vector length 2 specifying lower bounds scaling parameters (default c(0.1, 0.1)). upper numeric vector length 2 specifying upper bounds scaling parameters (default c(10, 10)). window numeric vector length 2 specifying time window restrict fixations `y` (default NULL, considers fixations).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/estimate_scale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Scaling Parameters for Fixation Data — estimate_scale","text":"list containing estimated scaling parameters.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.fixation_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute a density map for a fixation group. — eye_density.fixation_group","title":"Compute a density map for a fixation group. — eye_density.fixation_group","text":"function computes density map given fixation group using kernel density estimation.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.fixation_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute a density map for a fixation group. — eye_density.fixation_group","text":"","code":"# S3 method for class 'fixation_group' eye_density(   x,   sigma = 50,   xbounds = c(min(x$x), max(x$x)),   ybounds = c(min(x$y), max(x$y)),   outdim = c(xbounds[2] - xbounds[1], ybounds[2] - ybounds[1]),   normalize = TRUE,   duration_weighted = FALSE,   window = NULL,   min_fixations = 2,   origin = c(0, 0) )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.fixation_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute a density map for a fixation group. — eye_density.fixation_group","text":"x fixation_group object. sigma standard deviation kernel. Default 50. xbounds x-axis bounds. Default range x values fixation group. ybounds y-axis bounds. Default range y values fixation group. outdim output dimensions density map. Default difference xbounds ybounds. normalize Whether normalize output map. Default TRUE. duration_weighted Whether weight fixations duration. Default FALSE. window temporal window compute density map. Default NULL. min_fixations Minimum number fixations required compute density map. fewer present filtering, function returns NULL. Default 2. origin origin coordinate system. Default c(0,0).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.fixation_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute a density map for a fixation group. — eye_density.fixation_group","text":"object class c(\"eye_density\", \"density\", \"list\") containing computed density map relevant information.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.fixation_group.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute a density map for a fixation group. — eye_density.fixation_group","text":"function computes density map given fixation group using kde2d function MASS package. density map computed based x y coordinates fixations, optional weighting duration. resulting density map can normalized desired.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.html","id":null,"dir":"Reference","previous_headings":"","what":"eye_density — eye_density","title":"eye_density — eye_density","text":"Compute density map set eye fixations.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"eye_density — eye_density","text":"","code":"eye_density(x, sigma, xbounds, ybounds, outdim, weights, normalize, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"eye_density — eye_density","text":"x data frame containing eye fixations. sigma Numeric, standard deviation kernel used density estimation. xbounds Numeric vector length 2, defining minimum maximum x-axis bounds density map. ybounds Numeric vector length 2, defining minimum maximum y-axis bounds density map. outdim Numeric vector length 2, specifying number rows columns output density map. weights Optional, numeric vector fixation weights used density estimation. normalize Logical, whether normalize output density map values sum 1. ... Additional arguments passed method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"eye_density — eye_density","text":"object class \"eye_density\", \"density\", \"list\" containing computed density map relevant information.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"eye_density — eye_density","text":"function uses kernel density estimation compute density map eye fixations. takes various parameters control computation output density map.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct an Eye-Movement Data Frame — eye_table","title":"Construct an Eye-Movement Data Frame — eye_table","text":"`eye_table` function creates `data.frame` store eye-movement data columns x y coordinates, fixation duration, onset time, optional grouping variable. Additional variables can also retained. function filters data based clip bounds can compute relative coordinates.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct an Eye-Movement Data Frame — eye_table","text":"","code":"eye_table(   x,   y,   duration,   onset,   groupvar,   vars = NULL,   data,   clip_bounds = c(0, 1280, 0, 1280),   relative_coords = TRUE )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct an Eye-Movement Data Frame — eye_table","text":"x character symbol representing column x coordinates source data. y character symbol representing column y coordinates source data. duration character symbol representing column fixation durations source data. onset character symbol representing column fixation onset times source data. groupvar character symbol representing column grouping variable source data. vars character vector additional variable names retain, NULL (default) additional variables needed. data source `data.frame` containing eye-movement data. clip_bounds numeric vector length 4 representing clip bounds field view form c(xmin, xmax, ymin, ymax). Default c(0, 1280, 0, 1280). relative_coords logical value indicating whether compute relative coordinates (TRUE default). TRUE, x y coordinates transformed based clip_bounds.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct an Eye-Movement Data Frame — eye_table","text":"`data.frame` class \"eye_table\" columns x y coordinates, fixation duration, onset time, grouping variable, additional specified variables. data frame also \"origin\" attribute containing center coordinates field view.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/eye_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Construct an Eye-Movement Data Frame — eye_table","text":"`eye_table` function first checks input `data` `data.frame` renames columns specified x, y, duration, onset standard names. function filters data based specified clip_bounds, ensuring x y coordinates fall within bounds. relative_coords TRUE, x y coordinates transformed relative clip_bounds. function groups data specified grouping variable constructs fixation_group object group, added output data frame new \"fixgroup\" column. output data frame retains specified additional variables assigned class \"eye_table\". \"origin\" attribute output data frame contains center coordinates field view, computed based clip_bounds whether relative_coords TRUE FALSE.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixation Overlap Measure — fixation_overlap","title":"Fixation Overlap Measure — fixation_overlap","text":"Calculate number overlapping fixations two fixation groups, based minimum distance threshold, percentage overlapping fixations.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixation Overlap Measure — fixation_overlap","text":"","code":"fixation_overlap(   x,   y,   dthresh = 60,   time_samples = seq(0, max(x$onset), by = 20),   dist_method = c(\"euclidean\", \"manhattan\") )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fixation Overlap Measure — fixation_overlap","text":"x `fixation_group` object representing first fixation group. y `fixation_group` object representing second fixation group. dthresh numeric value specifying distance threshold determine two fixations overlap (default 60). time_samples numeric vector points time evaluate overlapping fixations (default `seq(0, max(x$onset), = 20)`). dist_method character string specifying distance metric use measuring distance fixations. Options \"euclidean\" \"manhattan\" (default \"euclidean\").","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fixation Overlap Measure — fixation_overlap","text":"list containing following elements: overlap number overlapping fixations two fixation groups. perc percentage overlapping fixations.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fixation Overlap Measure — fixation_overlap","text":"function computes number overlapping fixations within minimum distance two fixation groups specified time points. overlap occurs distance two fixations less specified distance threshold.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_overlap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fixation Overlap Measure — fixation_overlap","text":"","code":"# Create two fixation groups fg1 <- fixation_group(x = runif(50, 0, 100), y = runif(50, 0, 100), duration = rep(1, 50), onset = seq(1, 50)) fg2 <- fixation_group(x = runif(50, 0, 100), y = runif(50, 0, 100), duration = rep(1, 50), onset = seq(1, 50))  # Calculate the number of overlapping fixations and the percentage of overlapping fixations result <- fixation_overlap(fg1, fg2) overlap <- result$overlap #> Error in result$overlap: $ operator is invalid for atomic vectors perc <- result$perc #> Error in result$perc: $ operator is invalid for atomic vectors"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixation Similarity — fixation_similarity","title":"Fixation Similarity — fixation_similarity","text":"Compute similarity fixation group source_tab matching fixation group ref_tab.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixation Similarity — fixation_similarity","text":"","code":"fixation_similarity(   ref_tab,   source_tab,   match_on,   permutations = 0,   permute_on = NULL,   method = c(\"sinkhorn\", \"overlap\"),   refvar = \"fixgroup\",   sourcevar = \"fixgroup\",   window = NULL,   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fixation Similarity — fixation_similarity","text":"ref_tab reference table containing fixation groups compare. source_tab source table containing fixation groups compare. match_on column name tables used match fixation groups. permutations number permutations perform permutation tests (default 0, permutations). permute_on column name permute permutation tests (default NULL). method similarity metric use; options \"sinkhorn\" \"overlap\" (default \"sinkhorn\"). refvar name column containing fixation groups reference table (default \"fixgroup\"). sourcevar name column containing fixation groups source table (default \"fixgroup\"). window temporal window compute similarity (default NULL). ... Additional arguments pass similarity metric function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fixation Similarity — fixation_similarity","text":"table containing computed similarities fixation groups.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/fixation_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fixation Similarity — fixation_similarity","text":"","code":"# Example usage of the fixation_similarity function ref_table <- # reference table data source_table <- # source table data match_column <- # column name to match fixation groups similarity_results <- fixation_similarity(ref_table, source_table, match_column) #> fixation_similarity: similarity metric is sinkhornoverlap #> Error: object 'source_table' not found"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/gen_density.html","id":null,"dir":"Reference","previous_headings":"","what":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","title":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","text":"function creates density object provided x, y, z matrices. density object list containing x, y, z values class attribute set \"density\" \"list\".","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/gen_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","text":"","code":"gen_density(x, y, z)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/gen_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","text":"x numeric vector representing x-axis values density map. y numeric vector representing y-axis values density map. z matrix representing density values (x, y) coordinate.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/gen_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","text":"density object list containing x, y, z values class attribute set \"density\" \"list\".","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/gen_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"This function creates a density object from the provided x, y, and z matrices. The density object is a list containing the x, y, and z values with a class attribute set to ","text":"function first checks dimensions z matrix equal length x y vectors. , throws error. , creates list containing x, y, z values sets class attribute list \"density\" \"list\".","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/get_density.html","id":null,"dir":"Reference","previous_headings":"","what":"get_density — get_density","title":"get_density — get_density","text":"Get density matrix object.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/get_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get_density — get_density","text":"","code":"get_density(x, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/get_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get_density — get_density","text":"x object ... additional arguments","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/match_scale.html","id":null,"dir":"Reference","previous_headings":"","what":"Match Scaling Parameters for Fixation Data — match_scale","title":"Match Scaling Parameters for Fixation Data — match_scale","text":"function matches scaling parameters fixation data reference table source table based common matching variable.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/match_scale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Match Scaling Parameters for Fixation Data — match_scale","text":"","code":"match_scale(   ref_tab,   source_tab,   match_on,   refvar = \"fixgroup\",   sourcevar = \"fixgroup\",   window,   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/match_scale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Match Scaling Parameters for Fixation Data — match_scale","text":"ref_tab data frame containing reference fixation data. source_tab data frame containing source fixation data. match_on string specifying variable name `ref_tab` `source_tab` match . refvar string specifying variable name `ref_tab` containing reference fixations (default \"fixgroup\"). sourcevar string specifying variable name `source_tab` containing source fixations (default \"fixgroup\"). window numeric vector length 2 specifying time window restrict fixations source fixation data (default NULL, considers fixations). ... Additional arguments passed `estimate_scale` function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/match_scale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Match Scaling Parameters for Fixation Data — match_scale","text":"data frame containing original source fixation data additional columns matched scaling parameters.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/match_scale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Match Scaling Parameters for Fixation Data — match_scale","text":"","code":"# Example usage of the match_scale function ref_tab <- # reference fixation data source_tab <- # source fixation data matched_data <- match_scale(ref_tab, source_tab, match_on = \"subject_id\") #> Error in match_scale(ref_tab, source_tab, match_on = \"subject_id\"): could not find function \"match_scale\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/multi_match.html","id":null,"dir":"Reference","previous_headings":"","what":"multi_match — multi_match","title":"multi_match — multi_match","text":"multi_match compares two scanpaths based vector, direction, length, position, duration.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/multi_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"multi_match — multi_match","text":"","code":"multi_match(x, y, screensize)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/multi_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"multi_match — multi_match","text":"x first scanpath y second scanpath screensize two element vector indicating screen size","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize Eye-Movements to Unit Range — normalize","title":"Normalize Eye-Movements to Unit Range — normalize","text":"function normalizes eye-movements unit range based specified x y bounds.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize Eye-Movements to Unit Range — normalize","text":"","code":"normalize(x, xbounds, ybounds, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize Eye-Movements to Unit Range — normalize","text":"x input object containing eye-movements normalized. xbounds vector containing minimum maximum x bounds normalization. ybounds vector containing minimum maximum y bounds normalization. ... Additional arguments passed normalization method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize Eye-Movements to Unit Range — normalize","text":"object containing normalized eye-movements unit range.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize Eye-Movements to Unit Range — normalize","text":"","code":"# Example usage of the normalize function input_object <- # input object data containing eye-movements x_bounds <- c(0, 1000) # X bounds for normalization y_bounds <- c(0, 1000) # Y bounds for normalization normalized_object <- normalize(input_object, xbounds = x_bounds, ybounds = y_bounds) #> Error in UseMethod(\"normalize\", x): no applicable method for 'normalize' applied to an object of class \"c('double', 'numeric')\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.eye_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Eye Density — plot.eye_density","title":"Plot Eye Density — plot.eye_density","text":"function creates plot eye density using ggplot2.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.eye_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Eye Density — plot.eye_density","text":"","code":"# S3 method for class 'eye_density' plot(   x,   alpha = 0.8,   bg_image = NULL,   transform = c(\"identity\", \"sqroot\", \"curoot\", \"rank\"),   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.eye_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Eye Density — plot.eye_density","text":"x \"eye_density\" object. alpha opacity level raster layer (default: 0.8). bg_image optional image file name use background. transform transformation apply density values (default: c(\"identity\", \"sqroot\", \"curoot\", \"rank\")). ... Additional args","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.eye_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Eye Density — plot.eye_density","text":"ggplot object representing eye density plot.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.eye_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Eye Density — plot.eye_density","text":"","code":"# Assume `ed` is an \"eye_density\" object # Plot the eye density plot_eye_density <- plot.eye_density(ed) #> Error in plot.eye_density(ed): could not find function \"plot.eye_density\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.fixation_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a fixation_group object — plot.fixation_group","title":"Plot a fixation_group object — plot.fixation_group","text":"function creates plot fixation_group object using ggplot2. Different plot types options can specified customize output.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.fixation_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a fixation_group object — plot.fixation_group","text":"","code":"# S3 method for class 'fixation_group' plot(   x,   type = c(\"points\", \"contour\", \"filled_contour\", \"density\", \"raster\"),   bandwidth = 60,   xlim = range(x$x),   ylim = range(x$y),   size_points = TRUE,   show_points = TRUE,   show_path = TRUE,   bins = max(as.integer(length(x$x)/10), 4),   bg_image = NULL,   colours = rev(RColorBrewer.brewer.pal(n = 10, \"Spectral\")),   alpha_range = c(0.5, 1),   alpha = 0.8,   window = NULL,   transform = c(\"identity\", \"sqroot\", \"curoot\", \"rank\"),   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.fixation_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a fixation_group object — plot.fixation_group","text":"x fixation_group object. type type plot display (default: c(\"points\", \"contour\", \"filled_contour\", \"density\", \"raster\")). bandwidth bandwidth kernel density estimator (default: 60). xlim x-axis limits (default: range x values fixation_group object). ylim y-axis limits (default: range y values fixation_group object). size_points Whether size points according fixation duration (default: TRUE). show_points Whether show fixations points (default: TRUE). show_path Whether show fixation path (default: TRUE). bins Number bins density calculations (default: max(.integer(length(x$x)/10), 4)). bg_image optional background image file name. colours Color palette use plot (default: rev(RColorBrewer.brewer.pal(n=10, \"Spectral\"))). alpha_range vector specifying minimum maximum alpha values density plots (default: c(0.5, 1)). alpha opacity level points (default: 0.8). window vector specifying time window selecting fixations (default: NULL). transform transformation apply density values (default: c(\"identity\", \"sqroot\", \"curoot\", \"rank\")).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.fixation_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a fixation_group object — plot.fixation_group","text":"ggplot object representing fixation group plot.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/plot.fixation_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a fixation_group object — plot.fixation_group","text":"","code":"# Create a fixation_group object fg <- fixation_group(x=runif(50, 0, 100), y=runif(50, 0, 100), duration=rep(1,50), onset=seq(1,50)) # Plot the fixation group plot_fixation_group(fg) #> Error in plot_fixation_group(fg): could not find function \"plot_fixation_group\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rep_fixations.html","id":null,"dir":"Reference","previous_headings":"","what":"rep_fixations — rep_fixations","title":"rep_fixations — rep_fixations","text":"Replicate fixation sequence.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rep_fixations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"rep_fixations — rep_fixations","text":"","code":"rep_fixations(x, resolution)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rep_fixations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"rep_fixations — rep_fixations","text":"x object representing fixation sequence. resolution numeric value representing temporal resolution replicated fixations.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rep_fixations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"rep_fixations — rep_fixations","text":"object containing replicated fixation sequence specified temporal resolution.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rep_fixations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"rep_fixations — rep_fixations","text":"function replicates fixation sequence specified temporal resolution. can useful working fixation data needs resampled creating fixation sequences consistent temporal spacing.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/repetitive_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","title":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","text":"Computes within-condition -condition similarity density maps. density map (trial), function calculates average similarity maps within condition (`repsim`) average similarity maps different conditions (`othersim`).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/repetitive_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","text":"","code":"repetitive_similarity(   tab,   density_var = \"density\",   condition_var,   method = c(\"spearman\", \"pearson\", \"fisherz\", \"cosine\", \"l1\", \"jaccard\", \"dcov\", \"emd\"),   pairwise = FALSE,   multiscale_aggregation = \"mean\",   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/repetitive_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","text":"tab data frame tibble containing density maps condition identifiers. density_var character string specifying name column containing density maps (must class \"density\" compatible). Default \"density\". condition_var character string specifying name column identifying condition trial. method character string specifying similarity method use, passed `similarity.density`. Possible values include \"spearman\", \"pearson\", \"fisherz\", \"cosine\", \"l1\", \"jaccard\", \"dcov\". Default \"spearman\". pairwise logical value indicating whether return raw pairwise similarity scores within condition trial. Default FALSE. TRUE, list column named `pairwise_repsim` added. multiscale_aggregation densities multiscale (.e., `eye_density_multiscale` objects), specifies aggregate similarities different scales. Options include \"mean\" (default) \"none\" (get list column similarity vectors `pairwise_repsim`). Note: `repsim` `othersim` always report mean similarity across scales. ... Additional arguments passed `similarity.density` function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/repetitive_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","text":"input tibble `tab` augmented following columns:   - `repsim`: average similarity trial's density map maps within condition.   - `othersim`: average similarity trial's density map maps conditions.   - `pairwise_repsim` (optional, `pairwise = TRUE`): list column containing vectors similarity scores trial trial within condition.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/repetitive_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Repetitive Similarity Analysis for Density Maps — repetitive_similarity","text":"","code":"# \\donttest{   # Generate a small synthetic dataset of density maps across two conditions.   # Each \"density_map\" is created from normally-distributed random samples.   set.seed(123)   n_trials   <- 20   conditions <- rep(c(\"A\", \"B\"), each = n_trials / 2)    my_data <- tibble::tibble(     subject         = rep(1:4, length.out = n_trials),     trial_condition = conditions,     density_map     = purrr::map(seq_len(n_trials), function(i) {       x <- rnorm(100,                  mean = ifelse(conditions[i] == \"A\", 0, 2),                  sd   = 1)       stats::density(x)     })   )    # Compute within- and between-condition similarity.   result <- repetitive_similarity(     my_data,     density_var   = \"density_map\",     condition_var = \"trial_condition\",     method        = \"cosine\"   ) #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation.    # Optionally, return the raw pairwise similarities.   result_pairwise <- repetitive_similarity(     my_data,     density_var   = \"density_map\",     condition_var = \"trial_condition\",     method        = \"cosine\",     pairwise      = TRUE   ) #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. #> Warning: Less than 2 common valid data points for similarity calculation. # }"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rescale.html","id":null,"dir":"Reference","previous_headings":"","what":"rescale — rescale","title":"rescale — rescale","text":"Rescale spatial coordinates.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rescale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"rescale — rescale","text":"","code":"rescale(x, sx, sy)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rescale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"rescale — rescale","text":"x object containing spatial coordinates rescaled. sx numeric value representing x-axis scale factor. sy numeric value representing y-axis scale factor.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rescale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"rescale — rescale","text":"object rescaled spatial coordinates.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/rescale.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"rescale — rescale","text":"function rescales spatial coordinates object given scale factors.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/run_similarity_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Run similarity analysis for fixation data — run_similarity_analysis","title":"Run similarity analysis for fixation data — run_similarity_analysis","text":"function compares similarity fixation group source_tab matching fixation group ref_tab using specified similarity metric. Optionally, permutation tests can performed assessing significance similarity values.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/run_similarity_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run similarity analysis for fixation data — run_similarity_analysis","text":"","code":"run_similarity_analysis(   ref_tab,   source_tab,   match_on,   permutations,   permute_on = NULL,   method,   refvar,   sourcevar,   window = NULL,   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/run_similarity_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run similarity analysis for fixation data — run_similarity_analysis","text":"ref_tab data frame containing reference fixation groups. source_tab data frame containing source fixation groups compared reference fixation groups. match_on column name ref_tab source_tab used matching fixation groups. permutations number permutations perform assessing significance similarity values (default: 0, permutation tests). permute_on optional column name limiting matching indices permutation tests (default: NULL). method similarity metric use comparing fixation groups (e.g., \"sinkhorn\", \"overlap\"). refvar column name ref_tab containing reference fixation groups. sourcevar column name source_tab containing source fixation groups. window optional numeric vector specifying temporal window computing similarity (default: NULL). ... Extra arguments passed similarity function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.density.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","title":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","text":"function samples smooth fixation density map represented object x set discrete fixations provided fix.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","text":"","code":"# S3 method for class 'density' sample_density(x, fix, times = NULL)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","text":"x object class \"density\" representing smooth fixation density map. fix data frame tibble containing discrete fixations columns \"x\", \"y\", \"onset\". times vector numeric values representing time points density map sampled (default NULL).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","text":"data frame columns \"z\" \"time\", \"z\" contains sampled density values \"time\" contains corresponding time points.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample a smooth fixation density map with a set of discrete fixations. — sample_density.density","text":"function first checks times parameter NULL. , directly samples density map using coordinates fixations fix argument. times parameter provided, function first calls sample_fixations function generate new fixation sequence specified time points, samples density map using coordinates new fixation sequence. result data frame containing sampled density values corresponding time points.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.html","id":null,"dir":"Reference","previous_headings":"","what":"sample_density — sample_density","title":"sample_density — sample_density","text":"Sample smooth fixation density map set discrete fixations.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sample_density — sample_density","text":"","code":"sample_density(x, fix, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"sample_density — sample_density","text":"x object representing smooth fixation density map sampled. fix data frame containing discrete fixations used sampling density map. ... Additional arguments passed method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"sample_density — sample_density","text":"data frame columns 'z' (density estimates fixation locations) 'time' (onset time fixations).","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"sample_density — sample_density","text":"function samples given smooth fixation density map set discrete fixations estimate density locations fixations.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_fixations.html","id":null,"dir":"Reference","previous_headings":"","what":"sample_fixations — sample_fixations","title":"sample_fixations — sample_fixations","text":"sample_fixations","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_fixations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sample_fixations — sample_fixations","text":"","code":"sample_fixations(x, time)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sample_fixations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"sample_fixations — sample_fixations","text":"x fixation group time continuous time points sample","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.fixation_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","title":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","text":"function creates scanpath fixation group.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.fixation_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","text":"","code":"# S3 method for class 'fixation_group' scanpath(x, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.fixation_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","text":"x fixation group object.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.fixation_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","text":"scanpath object.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.fixation_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Scanpath for a Fixation Group — scanpath.fixation_group","text":"","code":"# Create a fixation group fixgroup <- data.frame(x = 1:5, y = 6:10) # Create a scanpath for the fixation group scanpath_obj <- scanpath.fixation_group(fixgroup) #> Error in scanpath.fixation_group(fixgroup): could not find function \"scanpath.fixation_group\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","title":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","text":"function creates scanpath containing polar coordinates (rho, theta) along absolute x y spatial coordinates given fixation group.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","text":"","code":"scanpath(x, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","text":"x fixations. ... Extra arguments.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","text":"scanpath object.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/scanpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Construct a Scanpath of a Fixation Group of Related Objects — scanpath","text":"","code":"# Create a fixation group fg <- fixation_group(x=c(.1,.5,1), y=c(1,.5,1), onset=1:3, duration=rep(1,3)) # Create a scanpath for the fixation group sp <- scanpath(fg)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Similarity Between Two Objects — similarity","title":"Compute Similarity Between Two Objects — similarity","text":"function computes similarity two objects using specified similarity metric.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Similarity Between Two Objects — similarity","text":"","code":"similarity(x, y, method, ...)"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Similarity Between Two Objects — similarity","text":"x first object compare. y second object compare. method character string specifying similarity metric used. ... Additional arguments passed similarity computation method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Similarity Between Two Objects — similarity","text":"numeric value representing similarity two input objects.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Similarity Between Two Objects — similarity","text":"","code":"# Example usage of the similarity function object1 <- # first object data object2 <- # second object data similarity_value <- similarity(object1, object2, method = \"some_method\") #> Error: object 'object1' not found"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.scanpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Similarity Between Scanpaths — similarity.scanpath","title":"Compute Similarity Between Scanpaths — similarity.scanpath","text":"function computes similarity two scanpaths using specified method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.scanpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Similarity Between Scanpaths — similarity.scanpath","text":"","code":"# S3 method for class 'scanpath' similarity(   x,   y,   method = c(\"multimatch\"),   window = NULL,   screensize = NULL,   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.scanpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Similarity Between Scanpaths — similarity.scanpath","text":"x scanpath object containing first scanpath. y scanpath object containing second scanpath. method character string specifying method compute similarity (default \"multimatch\"). window numeric vector length 2 specifying time window restrict fixations input scanpaths (default NULL, considers fixations). screensize numeric vector length 2 specifying dimensions screen (e.g., c(1000, 1000)). Required \"multimatch\" method. ... Additional arguments passed similarity computation method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.scanpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Similarity Between Scanpaths — similarity.scanpath","text":"numeric value representing similarity two input scanpaths.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/eyesim/reference/similarity.scanpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Similarity Between Scanpaths — similarity.scanpath","text":"","code":"# Example usage of the similarity.scanpath function scanpath1 <- # first scanpath data scanpath2 <- # second scanpath data similarity_value <- similarity.scanpath(scanpath1, scanpath2, method = \"multimatch\", screensize = c(1000, 1000)) #> Error in similarity.scanpath(scanpath1, scanpath2, method = \"multimatch\",     screensize = c(1000, 1000)): could not find function \"similarity.scanpath\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/simulate_eye_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","title":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","text":"`simulate_eye_table` function generates simulated `eye_table` object containing eye-movement data columns x y coordinates, fixation duration, onset time, optional grouping variable.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/simulate_eye_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","text":"","code":"simulate_eye_table(   n_fixations,   n_groups,   clip_bounds = c(0, 1280, 0, 1280),   relative_coords = TRUE )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/simulate_eye_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","text":"n_fixations number fixations simulate. n_groups number groups simulate. clip_bounds numeric vector length 4 representing clip bounds field view form c(xmin, xmax, ymin, ymax). Default c(0, 1280, 0, 1280). relative_coords logical value indicating whether compute relative coordinates (TRUE default). TRUE, x y coordinates transformed based clip_bounds.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/simulate_eye_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","text":"`data.frame` class \"eye_table\" simulated data.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/simulate_eye_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Simulated Eye-Movement Data Frame — simulate_eye_table","text":"","code":"sim_eye_table <- simulate_eye_table(n_fixations = 100, n_groups = 10) #> Error in select_at(., c(\"x\", \"y\", \"duration\", \"onset\", groupvar)): could not find function \"select_at\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sub-.eye_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Subset an 'eye_table' Object — [.eye_table","title":"Subset an 'eye_table' Object — [.eye_table","text":"`[.eye_table` function S3 method subsetting 'eye_table' objects. uses `NextMethod()` perform subsetting operation reapplies 'eye_table' class result using `as_eye_table` function. ensures returned object still 'eye_table' class subsetting.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sub-.eye_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subset an 'eye_table' Object — [.eye_table","text":"","code":"# S3 method for class 'eye_table' x[i, j, drop = FALSE]"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sub-.eye_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subset an 'eye_table' Object — [.eye_table","text":"x 'eye_table' object subsetted. Row indices subsetting. j Column indices subsetting. drop logical value indicating whether drop dimensions one level subsetting. Defaults `FALSE`.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sub-.eye_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subset an 'eye_table' Object — [.eye_table","text":"'eye_table' object subsetting.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/sub-.eye_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subset an 'eye_table' Object — [.eye_table","text":"","code":"# Create an 'eye_table' object df <- data.frame(x = 1:5, y = 6:10) eye_table_df <- as_eye_table(df)  # Subset the 'eye_table' object subset_eye_table <- eye_table_df[1:3,] class(subset_eye_table) #> [1] \"eye_table\"  \"data.frame\""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_multireg.html","id":null,"dir":"Reference","previous_headings":"","what":"Template Multiple Regression — template_multireg","title":"Template Multiple Regression — template_multireg","text":"function performs multiple regression provided source table specified response covariates using chosen regression method.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_multireg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Template Multiple Regression — template_multireg","text":"","code":"template_multireg(   source_tab,   response,   covars,   method = c(\"lm\", \"rlm\", \"nnls\", \"logistic\"),   intercept = TRUE )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_multireg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Template Multiple Regression — template_multireg","text":"source_tab data frame containing source maps. response character string specifying column name response variable. covars character vector specifying column names covariates. method character vector available regression methods. Default c(\"lm\", \"rlm\", \"nnls\", \"logistic\"). selected method used regression analysis. - \"lm\": Linear regression (default). - \"rlm\": Robust linear regression. - \"nnls\": Non-negative least squares. - \"logistic\": Logistic regression. intercept logical value indicating whether include intercept term model. Default TRUE.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_multireg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Template Multiple Regression — template_multireg","text":"data frame source table augmented new column, multireg, containing   results multiple regression analysis.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Template Regression — template_regression","title":"Template Regression — template_regression","text":"function performs template regression provided reference source tables estimate beta weights baseline source maps.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Template Regression — template_regression","text":"","code":"template_regression(   ref_tab,   source_tab,   match_on,   baseline_tab,   baseline_key,   method = c(\"lm\", \"rlm\", \"rank\") )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Template Regression — template_regression","text":"ref_tab data frame containing reference maps. source_tab data frame containing source maps. match_on character string specifying column name used matching reference source tables. baseline_tab data frame containing baseline maps. baseline_key character string specifying column name used matching baseline table source table. method character vector available regression methods. Default c(\"lm\", \"rlm\", \"rank\"). selected method used regression analysis. - \"lm\": Linear regression (default). - \"rlm\": Robust linear regression. - \"rank\": Rank-based correlation.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Template Regression — template_regression","text":"data frame source table augmented two new columns, beta_baseline beta_source,   representing estimated beta weights baseline source maps, respectively.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample density maps with coordinates derived from fixation groups. — template_sample","title":"Sample density maps with coordinates derived from fixation groups. — template_sample","text":"function extracts density arbitrary time point trial. simply extracts value density map fixation time t. fixations taken `fixgroup` variable may associated, example, independent set trials.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample density maps with coordinates derived from fixation groups. — template_sample","text":"","code":"template_sample(   source_tab,   template,   fixgroup = \"fixgroup\",   time = NULL,   outcol = \"sample_out\" )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample density maps with coordinates derived from fixation groups. — template_sample","text":"source_tab name table containing density map fixations template name template density variable fixgroup name fixation group supplying spatiotemporal coordinates used sample template time time points used extract coordinates `fixation_group` outcol name output variable","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"template_similarity — template_similarity","title":"template_similarity — template_similarity","text":"Compute similarity density map source_tab matching (\"template\") density map ref_tab.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"template_similarity — template_similarity","text":"","code":"template_similarity(   ref_tab,   source_tab,   match_on,   permute_on = NULL,   refvar = \"density\",   sourcevar = \"density\",   method = c(\"spearman\", \"pearson\", \"fisherz\", \"cosine\", \"l1\", \"jaccard\", \"dcov\", \"emd\"),   permutations = 10,   ... )"},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"template_similarity — template_similarity","text":"ref_tab data frame tibble containing reference density maps. source_tab data frame tibble containing source density maps. match_on character string representing variable used match density maps ref_tab source_tab. permute_on character string representing variable used stratify permutations (default NULL). refvar character string representing name variable containing density maps reference table (default \"density\"). sourcevar character string representing name variable containing density maps source table (default \"density\"). method character string specifying similarity method use. Possible values \"spearman\", \"pearson\", \"fisherz\", \"cosine\", \"l1\", \"jaccard\", \"dcov\", \"emd\" (default \"spearman\"). permutations numeric value specifying number permutations baseline map (default 10). ... Extra arguments pass `similarity` function.","code":""},{"path":"https://bbuchsbaum.github.io/eyesim/reference/template_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"template_similarity — template_similarity","text":"data frame tibble containing source table additional columns similarity scores permutation results.","code":""}]
